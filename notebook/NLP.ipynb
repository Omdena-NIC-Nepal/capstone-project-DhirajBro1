{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df36318c",
   "metadata": {},
   "source": [
    "### Collecting News,article on Climate Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8976c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Fetching article metadata...\n",
      "INFO: Found 50 articles. Starting content extraction...\n",
      "INFO: ====== WebDriver manager ======\n",
      "INFO: Get LATEST chromedriver version for google-chrome\n",
      "INFO: Get LATEST chromedriver version for google-chrome\n",
      "INFO: There is no [win64] chromedriver \"135.0.7049.114\" for browser google-chrome \"135.0.7049\" in cache\n",
      "INFO: Get LATEST chromedriver version for google-chrome\n",
      "INFO: WebDriver version 135.0.7049.114 selected\n",
      "INFO: Modern chrome version https://storage.googleapis.com/chrome-for-testing-public/135.0.7049.114/win32/chromedriver-win32.zip\n",
      "INFO: About to download new driver from https://storage.googleapis.com/chrome-for-testing-public/135.0.7049.114/win32/chromedriver-win32.zip\n",
      "INFO: Driver downloading response is 200\n",
      "INFO: Get LATEST chromedriver version for google-chrome\n",
      "INFO: Driver has been saved in cache [C:\\Users\\SHYAM PANDIT\\.wdm\\drivers\\chromedriver\\win64\\135.0.7049.114]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]INFO: ðŸ“„ Processing: Climate Change Amplified the Effects of Extreme Rainfall in Nepal - eos.org\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Climate_Change_Amplified_the_Effects_of_Extreme_Rainfall_in_Nepal___eos_org.txt\n",
      "  2%|â–         | 1/50 [04:56<4:02:28, 296.92s/it]INFO: ðŸ“„ Processing: Nepal round three | UNC-Chapel Hill - The University of North Carolina at Chapel Hill\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Nepal_round_three___UNC_Chapel_Hill___The_University_of_North_Carolina_at_Chapel.txt\n",
      "  4%|â–         | 2/50 [06:54<2:33:19, 191.65s/it]INFO: ðŸ“„ Processing: Health Heroes of the Himalayas: Vaccine Delivery on the Roof of the World - Angels in Medicine\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Health_Heroes_of_the_Himalayas__Vaccine_Delivery_on_the_Roof_of_the_World___Ange.txt\n",
      "  6%|â–Œ         | 3/50 [07:17<1:29:39, 114.45s/it]INFO: ðŸ“„ Processing: The era of climate crisis in Nepal: A call for urgent action - CGIAR\n",
      "INFO: âœ… Extracted using XPath: //section[contains(@class, \"content\")]\n",
      "INFO: âœ… Saved: The_era_of_climate_crisis_in_Nepal__A_call_for_urgent_action___CGIAR.txt\n",
      "  8%|â–Š         | 4/50 [07:33<57:49, 75.43s/it]   INFO: ðŸ“„ Processing: Rapid urbanisation and climate change key drivers of dramatic flood impacts in Nepal - World Weather Attribution\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Rapid_urbanisation_and_climate_change_key_drivers_of_dramatic_flood_impacts_in_N.txt\n",
      " 10%|â–ˆ         | 5/50 [07:43<39:01, 52.03s/it]INFO: ðŸ“„ Processing: Climate change worsened deadly Nepal floods, scientists say - Phys.org\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Climate_change_worsened_deadly_Nepal_floods__scientists_say___Phys_org.txt\n",
      " 12%|â–ˆâ–        | 6/50 [07:49<26:37, 36.30s/it]INFO: ðŸ“„ Processing: Nepalâ€™s Prime Minister highlights climate crisis, global inequality at UNGA - UN News\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMiV0FVX3lxTFB6aUJYRjBXVHI0VFczVC1jVEhWZjl3WWczRzlRMmZwblNySVBQdmVxZUdXOTBUYldrTDAwQ1U0dFFybXBsOWRwVjMyUWdPWkVya1FrZzlEVQ?oc=5\n",
      "INFO: ðŸš« Could not extract content: Nepalâ€™s Prime Minister highlights climate crisis, global inequality at UNGA - UN News\n",
      " 14%|â–ˆâ–        | 7/50 [08:23<25:24, 35.44s/it]INFO: ðŸ“„ Processing: Migration, Environment and Climate Change Nexus in Nepal: Policy Brief [EN/NE] - ReliefWeb\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Migration__Environment_and_Climate_Change_Nexus_in_Nepal__Policy_Brief__EN_NE___.txt\n",
      " 16%|â–ˆâ–Œ        | 8/50 [08:42<21:17, 30.42s/it]INFO: ðŸ“„ Processing: Climate Change Made Nepal Floods 70% More Likely, Experts Say - Earth.Org\n",
      "INFO: âœ… Extracted using XPath: //section[contains(@class, \"content\")]\n",
      "INFO: âœ… Saved: Climate_Change_Made_Nepal_Floods_70__More_Likely__Experts_Say___Earth_Org.txt\n",
      " 18%|â–ˆâ–Š        | 9/50 [09:08<19:42, 28.84s/it]INFO: ðŸ“„ Processing: â€˜Our world has endedâ€™: Nepal floods devastate lives and the economy - South China Morning Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: _Our_world_has_ended___Nepal_floods_devastate_lives_and_the_economy___South_Chin.txt\n",
      " 20%|â–ˆâ–ˆ        | 10/50 [09:59<23:53, 35.83s/it]INFO: ðŸ“„ Processing: Climate change made downpours behind deadly Nepal floods 10% more intense â€“ Study - Red Cross Climate Centre\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Climate_change_made_downpours_behind_deadly_Nepal_floods_10__more_intense___Stud.txt\n",
      " 22%|â–ˆâ–ˆâ–       | 11/50 [10:36<23:35, 36.31s/it]INFO: ðŸ“„ Processing: Nepalâ€™s deadly floods trigger calls for climate adaptation - Mongabay\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Nepal_s_deadly_floods_trigger_calls_for_climate_adaptation___Mongabay.txt\n",
      " 24%|â–ˆâ–ˆâ–       | 12/50 [12:21<36:05, 56.98s/it]INFO: ðŸ“„ Processing: Nepal says China withholds â€œessentialâ€ info on bursting Himalayan glacial lakes - Climate Home News\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Nepal_says_China_withholds__essential__info_on_bursting_Himalayan_glacial_lakes_.txt\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [13:04<32:34, 52.82s/it]INFO: ðŸ“„ Processing: Children and Youth at the Center of Nepalâ€™s Climate Change Agenda - radionepalonline.com\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Children_and_Youth_at_the_Center_of_Nepal_s_Climate_Change_Agenda___radionepalon.txt\n",
      " 28%|â–ˆâ–ˆâ–Š       | 14/50 [14:55<42:17, 70.47s/it]INFO: ðŸ“„ Processing: Manipur journalists learn climate change reporting techniques at ICIMOD Nepal - India Today NE\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Manipur_journalists_learn_climate_change_reporting_techniques_at_ICIMOD_Nepal___.txt\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [15:04<30:11, 51.76s/it]INFO: ðŸ“„ Processing: Application Open for a Capacity Building and Reporting Opportunity on Climate Change and DRR in the Nepal-India Context - UNESCO\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Application_Open_for_a_Capacity_Building_and_Reporting_Opportunity_on_Climate_Ch.txt\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [15:28<24:38, 43.47s/it]INFO: ðŸ“„ Processing: In Nepal, climate change is shadowed by two decades of surging dengue - Gavi, the Vaccine Alliance\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: In_Nepal__climate_change_is_shadowed_by_two_decades_of_surging_dengue___Gavi__th.txt\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [17:14<34:12, 62.20s/it]INFO: ðŸ“„ Processing: Climate emergency: 2025 declared international year of glaciers - UN News\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMiV0FVX3lxTFBhYUM4b0NWa2JlUHRMUFpCV0ZPY25YMmlwX2hYS2wtRjJVVG5jNmgtb1FTb0ZTV3I3T25zMG50aDl4dThEYjNmSFlZTWF1TTl5NDlYSXgxbw?oc=5\n",
      "INFO: ðŸš« Could not extract content: Climate emergency: 2025 declared international year of glaciers - UN News\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [17:40<27:30, 51.58s/it]INFO: ðŸ“„ Processing: Climate change could erase Nepalâ€™s snow leopards in three decades, study warns - South China Morning Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Climate_change_could_erase_Nepal_s_snow_leopards_in_three_decades__study_warns__.txt\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [18:28<26:02, 50.40s/it]INFO: ðŸ“„ Processing: Through Climate Diplomacy, Nepal Sounds the Alarm on the Himalayas - The Diplomat â€“ Asia-Pacific Current Affairs Magazine\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Through_Climate_Diplomacy__Nepal_Sounds_the_Alarm_on_the_Himalayas___The_Diploma.txt\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [18:45<20:07, 40.25s/it]INFO: ðŸ“„ Processing: Activists blame policy failures over climate change for Nepal Teraiâ€™s water crisis - Mongabay\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Activists_blame_policy_failures_over_climate_change_for_Nepal_Terai_s_water_cris.txt\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [20:31<29:04, 60.15s/it]INFO: ðŸ“„ Processing: Rethinking Climate Finance to Work for Women in Nepalâ€™s Coffee and Pashmina Value Chains - ESCAP\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Rethinking_Climate_Finance_to_Work_for_Women_in_Nepal_s_Coffee_and_Pashmina_Valu.txt\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [20:51<22:24, 48.01s/it]INFO: ðŸ“„ Processing: Ecological vulnerability and driving factors in the himalayan transboundary landscape under global climate change - Nature\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Ecological_vulnerability_and_driving_factors_in_the_himalayan_transboundary_land.txt\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [21:24<19:34, 43.49s/it]INFO: ðŸ“„ Processing: Nepalâ€™s climate change issue raised in UK Parliament - The Kathmandu Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMimwFBVV95cUxPaDhEUlowVGd4ckRHUnhtQ3ZBd2lLajJ4YllkVHpzOTZuZS1zQ2VIUWtsSkFzMnJQT2I0aDNFZUctVEc4UG1BN1pGc3Y1N0ROTWNWa0JvbVNEOWdQU3ZVM1BzbFRuWW41NHZQdlVQOGZSaHlPQ3dFV3NfZllDdXJ6V2NYUkJ0dkltOWxGUzRLRVBUUHlYOXZnRGFDUQ?oc=5\n",
      "INFO: ðŸš« Could not extract content: Nepalâ€™s climate change issue raised in UK Parliament - The Kathmandu Post\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [21:47<16:15, 37.51s/it]INFO: ðŸ“„ Processing: Wildfires are raging in Nepal â€” climate change isnâ€™t the only culprit - Nature\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Wildfires_are_raging_in_Nepal___climate_change_isn_t_the_only_culprit___Nature.txt\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [22:08<13:33, 32.55s/it]INFO: ðŸ“„ Processing: Climate migration amplifies gender inequalities - Nepal - ReliefWeb\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Climate_migration_amplifies_gender_inequalities___Nepal___ReliefWeb.txt\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [22:24<10:57, 27.40s/it]INFO: ðŸ“„ Processing: â€œNot Just a Climate Crisis, But a Justice Crisisâ€: Women in rural Nepal speak out - myRepublica\n",
      "INFO: âœ… Extracted using XPath: //div[@id=\"content\"]\n",
      "INFO: âœ… Saved: _Not_Just_a_Climate_Crisis__But_a_Justice_Crisis___Women_in_rural_Nepal_speak_ou.txt\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [22:45<09:48, 25.61s/it]INFO: ðŸ“„ Processing: Enhancing the resilience of food production systems for food and nutritional security under climate change in Nepal - Frontiers\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Enhancing_the_resilience_of_food_production_systems_for_food_and_nutritional_sec.txt\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [23:11<09:26, 25.73s/it]INFO: ðŸ“„ Processing: Nepal should see climate change as an opportunity - The Kathmandu Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMiogFBVV95cUxPU2NlZmRNYUhxZ1pfcXQzc3NVXzhUM3YyUmxKS0VJZWpqWVJIY0FQcW9nd3VhV2VIeDl4empBSWY5TXBLbGptOGxndFRiSmw2eW1XaFpkcThUTVg3RHlkX3RsNE04STZTUXZ2ZV9QTHVlcGFmLS1aMi1ZSXo5Y090VkEyNU8wWE9ndFlUN3Qtd1F2TmoxSlhETmxDcWxzTy1sQ3c?oc=5\n",
      "INFO: ðŸš« Could not extract content: Nepal should see climate change as an opportunity - The Kathmandu Post\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [23:31<08:21, 23.86s/it]INFO: ðŸ“„ Processing: Gender Integration in Climate Change and Agricultural Policies: The Case of Nepal - Frontiers\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Gender_Integration_in_Climate_Change_and_Agricultural_Policies__The_Case_of_Nepa.txt\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [23:47<07:13, 21.66s/it]INFO: ðŸ“„ Processing: â€˜This app saved my farm:â€™ How technology is helping Nepali farmers fight climate change - Islamic Relief Worldwide\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: _This_app_saved_my_farm___How_technology_is_helping_Nepali_farmers_fight_climate.txt\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [24:13<07:16, 22.95s/it]INFO: ðŸ“„ Processing: BaYu Sambaad: Centering Children and Youth in Nepalâ€™s Climate Agenda â€“ Government of Nepal, United Nations in Nepal - Unicef\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: BaYu_Sambaad__Centering_Children_and_Youth_in_Nepal_s_Climate_Agenda___Governmen.txt\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [24:54<08:27, 28.18s/it]INFO: ðŸ“„ Processing: Strengthening a Climate Smart, Green, and Resilient Education System in Nepal - UNESCO\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Strengthening_a_Climate_Smart__Green__and_Resilient_Education_System_in_Nepal___.txt\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [25:19<07:47, 27.47s/it]INFO: ðŸ“„ Processing: Nepal is bearing brunt of climate change impacts in disproportionate manner: Foreign Minister Deuba - The Kathmandu Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMi3AFBVV95cUxOM1o1ODdHUUFFQkhsUmFBQVBKeUQxOVhjQXVnS2FvNG0xY0xhRnViWWJWSTZKdmdvZ0s3djI1NFNaM0c0TmJTVm9ybE1lUXNtQlJSTDl6cmRvUHdtOENsdkNRdHR6ck9OSENTWDM4alFyQnRRTFVCZXEwVFFPbzdhdWlkVkN4X3VBQnFVX09CdVNoSDhxdVFqbmpWbXZ3QU1ZOHdxZlZIZ0M2NU1mX1Q4S0VYZ0hQbmdjRWlYd0MxX0dhYTFJLTNVZWVzcG15cVpYSVRhZldpbFlZQTBD?oc=5\n",
      "INFO: ðŸš« Could not extract content: Nepal is bearing brunt of climate change impacts in disproportionate manner: Foreign Minister Deuba - The Kathmandu Post\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [25:36<06:28, 24.25s/it]INFO: ðŸ“„ Processing: How climate change has hit Nepali kitchens - Asia News Network\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: How_climate_change_has_hit_Nepali_kitchens___Asia_News_Network.txt\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [25:46<04:59, 19.94s/it]INFO: ðŸ“„ Processing: Those Who Stay: Weathering the Consequences of a Climate Breakdown at the Top of the World - Pulitzer Center\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Those_Who_Stay__Weathering_the_Consequences_of_a_Climate_Breakdown_at_the_Top_of.txt\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [26:03<04:24, 18.89s/it]INFO: ðŸ“„ Processing: Nepalâ€™s Mountain Communities Contemplate the End of â€˜Himalayan Goldâ€™ - The Diplomat â€“ Asia-Pacific Current Affairs Magazine\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Nepal_s_Mountain_Communities_Contemplate_the_End_of__Himalayan_Gold____The_Diplo.txt\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [26:17<03:50, 17.71s/it]INFO: ðŸ“„ Processing: Living with climate change - Unicef\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Living_with_climate_change___Unicef.txt\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [27:18<06:05, 30.46s/it]INFO: ðŸ“„ Processing: At International Court of Justice, Nepal demands climate justice - The Kathmandu Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMivAFBVV95cUxQZDByMXl6OFhLQWxkck1HMjVuMEpMNlJNZ3VSMlhuQVR0N1FyelJKZmZxNG5ob1dYbm9fTFlHOXdKQzlnaUQxMUcxaE52ZWloblJLNkk3SnhiX1lhX0FyaURHMzlOelFWRmR6bHVKNWIzVlNvb09SWG1TaDFOLU9NSTJmV1ZGYkVkRVdqT0laenQ1RDkzaklKQTYtZjU3QmFabGVmRXA2MUltTXQzY2VGWkVPV0VielBnNnJIcg?oc=5\n",
      "INFO: ðŸš« Could not extract content: At International Court of Justice, Nepal demands climate justice - The Kathmandu Post\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [27:41<05:11, 28.27s/it]INFO: ðŸ“„ Processing: Why are Nepalâ€™s climate plans stalling? Lack of funds and manpower, say stakeholders - The Kathmandu Post\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âš ï¸  Falling back to newspaper3k for: https://news.google.com/rss/articles/CBMi1gFBVV95cUxNMXBNbVM3NllIQXFteHMxQ0VzMzlnYThWSHZyY0FrbTNqc3JRTmFDZW50TzZ5cmRJNUxQNWl2cXBfWnk3Slh5endjNzF1eUNadjFPZXdUVUhhbjZlcC1KRnF6VjY1S0RPdUk4ZHFZRTI4ZkMzYmY4X3ByeENueV9mSjJud2I4dF9pSEtVRGdVMnF3c2QzLXhFQ3R4YjR0TGRRRmhNOGtnaUt6X2RGaXItZG1Sb3EydUNSZjZ2MFVMVFZaTUVUbGh4dVI4Z0kwWVMxZlJPUTBn?oc=5\n",
      "INFO: ðŸš« Could not extract content: Why are Nepalâ€™s climate plans stalling? Lack of funds and manpower, say stakeholders - The Kathmandu Post\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [27:56<04:04, 24.40s/it]INFO: ðŸ“„ Processing: Nepal - Country Climate and Development Report - International Finance Corporation (IFC)\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Nepal___Country_Climate_and_Development_Report___International_Finance_Corporati.txt\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [28:14<03:20, 22.26s/it]INFO: ðŸ“„ Processing: Climate change and its silent impact on sexual and reproductive health in Nepal - ipas.org\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Climate_change_and_its_silent_impact_on_sexual_and_reproductive_health_in_Nepal_.txt\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [28:50<03:31, 26.44s/it]INFO: ðŸ“„ Processing: Indigenous people's perception of indigenous agricultural knowledge for climate change adaptation in Khumbu, Nepal - Frontiers\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Indigenous_people_s_perception_of_indigenous_agricultural_knowledge_for_climate_.txt\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [29:13<02:57, 25.36s/it]INFO: ðŸ“„ Processing: ADB Approves $30 Million Financing to Strengthen Climate Resilience in Nepal - Asian Development Bank\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: ADB_Approves__30_Million_Financing_to_Strengthen_Climate_Resilience_in_Nepal___A.txt\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [29:39<02:33, 25.54s/it]INFO: ðŸ“„ Processing: In Nepal, climate change threatens honey hunters' tradition - DW\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: In_Nepal__climate_change_threatens_honey_hunters__tradition___DW.txt\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [29:56<01:55, 23.16s/it]INFO: ðŸ“„ Processing: A village in Nepal faces the wrath of climate change - Asia News Network\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: A_village_in_Nepal_faces_the_wrath_of_climate_change___Asia_News_Network.txt\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [30:08<01:19, 19.82s/it]INFO: ðŸ“„ Processing: United In the Face of Disaster: Nepal Navigates Climate Risks - United Nations Sustainable Development Group\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: United_In_the_Face_of_Disaster__Nepal_Navigates_Climate_Risks___United_Nations_S.txt\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [30:27<00:58, 19.63s/it]INFO: ðŸ“„ Processing: Research on Human Security-Centred and Gender-Responsive Migration, Environment and Climate Change in Nepal - ReliefWeb\n",
      "INFO: âœ… Extracted using XPath: //article\n",
      "INFO: âœ… Saved: Research_on_Human_Security_Centred_and_Gender_Responsive_Migration__Environment_.txt\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [30:55<00:44, 22.07s/it]INFO: ðŸ“„ Processing: Nepal rains intensified by human-induced climate change: scientists - National Herald\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: âœ… Saved: Nepal_rains_intensified_by_human_induced_climate_change__scientists___National_H.txt\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [33:52<01:08, 68.48s/it]INFO: ðŸ“„ Processing: Human induced climate change made Nepal rains more intense, scientists say - The Economic Times\n",
      "INFO: âš ï¸  XPath selectors failed. Using full page source.\n",
      "INFO: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173/source\n",
      "INFO: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173/source\n",
      "INFO: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173/source\n",
      "INFO: âŒ Exception extracting from https://news.google.com/rss/articles/CBMi7gFBVV95cUxNZHNPRGJoamc4dXdfMUE0MkNfLUVFU3h3ZXJSVVJIQ040ZElVWXo2cnNyb1VFRS1vMkpUNDFKZkJKZUtmdUVmaU5ZQ0hpZTYtY2lzWWFhTjVZRzBHVVNzeDNKdHZZZklFZ19DODh6MFhJNlRMTzBvNGFhbHNPUU5NMlJfbFNMemR6TWFOOGY2TVRsQldpNzh6WHk0N2tGMnFfRFEzTk8tRWZCcXZqeWRxOFI0ZlBLQnBid1hMRWtkNHNhUGVNRFlOdGZXY3pFak15WERxblRuNHJHSXZxMDNibC1KZFNOM2M3WWdwRG1n0gHzAUFVX3lxTE9QajZJSHBpTlNWUUZSUGdLSk8yUlBpSFNaNy1Mb3ljR3pyaWFtbXBYOFdUa0tKWUZrZnJxQnUtQ25MZUZ1UmhpdU5kUUk4amhqUnN2Yl9OaWx0OS1pVk5xNnRUbDFjeGFFS00yaGhfeWxYbW5FQmlCQ29PZ1ZoVUxCcEtCOEtfVzd0NXVkWkUyQzhvcmhWcDRudGVZMlZ6U3dIMENvd1cwdXdjOEFtM3E5LUJ0YnhleUFOOEVhbG83ZmwxXzZDdzdDYlMxOXk3bVRobnJZN0Z4X050SlczMWJoNHdoMnI3NXR0eVMzcUdkTDFucw?oc=5: HTTPConnectionPool(host='localhost', port=63256): Max retries exceeded with url: /session/1b2169b4e015ebeff71f02900510d173/source (Caused by ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\"))\n",
      "INFO: ðŸš« Could not extract content: Human induced climate change made Nepal rains more intense, scientists say - The Economic Times\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [52:19<00:00, 62.79s/it] \n",
      "INFO: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173\n",
      "INFO: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173\n",
      "INFO: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPConnectionPool(host='localhost', port=63256): Read timed out. (read timeout=120)\")': /session/1b2169b4e015ebeff71f02900510d173\n",
      "INFO: \n",
      "ðŸ§¾ Failed Articles:\n",
      "INFO:  - https://news.google.com/rss/articles/CBMiV0FVX3lxTFB6aUJYRjBXVHI0VFczVC1jVEhWZjl3WWczRzlRMmZwblNySVBQdmVxZUdXOTBUYldrTDAwQ1U0dFFybXBsOWRwVjMyUWdPWkVya1FrZzlEVQ?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMiV0FVX3lxTFBhYUM4b0NWa2JlUHRMUFpCV0ZPY25YMmlwX2hYS2wtRjJVVG5jNmgtb1FTb0ZTV3I3T25zMG50aDl4dThEYjNmSFlZTWF1TTl5NDlYSXgxbw?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMimwFBVV95cUxPaDhEUlowVGd4ckRHUnhtQ3ZBd2lLajJ4YllkVHpzOTZuZS1zQ2VIUWtsSkFzMnJQT2I0aDNFZUctVEc4UG1BN1pGc3Y1N0ROTWNWa0JvbVNEOWdQU3ZVM1BzbFRuWW41NHZQdlVQOGZSaHlPQ3dFV3NfZllDdXJ6V2NYUkJ0dkltOWxGUzRLRVBUUHlYOXZnRGFDUQ?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMiogFBVV95cUxPU2NlZmRNYUhxZ1pfcXQzc3NVXzhUM3YyUmxKS0VJZWpqWVJIY0FQcW9nd3VhV2VIeDl4empBSWY5TXBLbGptOGxndFRiSmw2eW1XaFpkcThUTVg3RHlkX3RsNE04STZTUXZ2ZV9QTHVlcGFmLS1aMi1ZSXo5Y090VkEyNU8wWE9ndFlUN3Qtd1F2TmoxSlhETmxDcWxzTy1sQ3c?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMi3AFBVV95cUxOM1o1ODdHUUFFQkhsUmFBQVBKeUQxOVhjQXVnS2FvNG0xY0xhRnViWWJWSTZKdmdvZ0s3djI1NFNaM0c0TmJTVm9ybE1lUXNtQlJSTDl6cmRvUHdtOENsdkNRdHR6ck9OSENTWDM4alFyQnRRTFVCZXEwVFFPbzdhdWlkVkN4X3VBQnFVX09CdVNoSDhxdVFqbmpWbXZ3QU1ZOHdxZlZIZ0M2NU1mX1Q4S0VYZ0hQbmdjRWlYd0MxX0dhYTFJLTNVZWVzcG15cVpYSVRhZldpbFlZQTBD?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMivAFBVV95cUxQZDByMXl6OFhLQWxkck1HMjVuMEpMNlJNZ3VSMlhuQVR0N1FyelJKZmZxNG5ob1dYbm9fTFlHOXdKQzlnaUQxMUcxaE52ZWloblJLNkk3SnhiX1lhX0FyaURHMzlOelFWRmR6bHVKNWIzVlNvb09SWG1TaDFOLU9NSTJmV1ZGYkVkRVdqT0laenQ1RDkzaklKQTYtZjU3QmFabGVmRXA2MUltTXQzY2VGWkVPV0VielBnNnJIcg?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMi1gFBVV95cUxNMXBNbVM3NllIQXFteHMxQ0VzMzlnYThWSHZyY0FrbTNqc3JRTmFDZW50TzZ5cmRJNUxQNWl2cXBfWnk3Slh5endjNzF1eUNadjFPZXdUVUhhbjZlcC1KRnF6VjY1S0RPdUk4ZHFZRTI4ZkMzYmY4X3ByeENueV9mSjJud2I4dF9pSEtVRGdVMnF3c2QzLXhFQ3R4YjR0TGRRRmhNOGtnaUt6X2RGaXItZG1Sb3EydUNSZjZ2MFVMVFZaTUVUbGh4dVI4Z0kwWVMxZlJPUTBn?oc=5\n",
      "INFO:  - https://news.google.com/rss/articles/CBMi7gFBVV95cUxNZHNPRGJoamc4dXdfMUE0MkNfLUVFU3h3ZXJSVVJIQ040ZElVWXo2cnNyb1VFRS1vMkpUNDFKZkJKZUtmdUVmaU5ZQ0hpZTYtY2lzWWFhTjVZRzBHVVNzeDNKdHZZZklFZ19DODh6MFhJNlRMTzBvNGFhbHNPUU5NMlJfbFNMemR6TWFOOGY2TVRsQldpNzh6WHk0N2tGMnFfRFEzTk8tRWZCcXZqeWRxOFI0ZlBLQnBid1hMRWtkNHNhUGVNRFlOdGZXY3pFak15WERxblRuNHJHSXZxMDNibC1KZFNOM2M3WWdwRG1n0gHzAUFVX3lxTE9QajZJSHBpTlNWUUZSUGdLSk8yUlBpSFNaNy1Mb3ljR3pyaWFtbXBYOFdUa0tKWUZrZnJxQnUtQ25MZUZ1UmhpdU5kUUk4amhqUnN2Yl9OaWx0OS1pVk5xNnRUbDFjeGFFS00yaGhfeWxYbW5FQmlCQ29PZ1ZoVUxCcEtCOEtfVzd0NXVkWkUyQzhvcmhWcDRudGVZMlZ6U3dIMENvd1cwdXdjOEFtM3E5LUJ0YnhleUFOOEVhbG83ZmwxXzZDdzdDYlMxOXk3bVRobnJZN0Z4X050SlczMWJoNHdoMnI3NXR0eVMzcUdkTDFucw?oc=5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"INFO: %(message)s\")\n",
    "\n",
    "# Directory to save articles\n",
    "SAVE_DIR = \"articles\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# RSS feed of climate-related news\n",
    "RSS_FEED = \"https://news.google.com/rss/search?q=climate+change+Nepal&hl=en-NE&gl=NP&ceid=NP:en\"\n",
    "\n",
    "# Setup headless Chrome WebDriver\n",
    "def get_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Clean extracted HTML content using BeautifulSoup\n",
    "def clean_article_content(raw_html: str) -> str:\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    # Remove unnecessary sections\n",
    "    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'aside']):\n",
    "        tag.decompose()\n",
    "\n",
    "    for a in soup.find_all('a', string=re.compile(r'(Facebook|LinkedIn|X|Twitter|Share)', re.IGNORECASE)):\n",
    "        a.decompose()\n",
    "\n",
    "    for div in soup.find_all('div', class_=re.compile(r'download|attachments|related', re.IGNORECASE)):\n",
    "        div.decompose()\n",
    "\n",
    "    # Collect meaningful blocks\n",
    "    blocks = soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'blockquote'])\n",
    "\n",
    "    # Join text with paragraph spacing\n",
    "    content = '\\n\\n'.join(block.get_text(strip=True) for block in blocks if block.get_text(strip=True))\n",
    "    return re.sub(r'\\s{3,}', '\\n\\n', content).strip()\n",
    "\n",
    "# Extract article content using Selenium and fallback to newspaper3k\n",
    "def extract_article_content(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        selectors = [\n",
    "            '//article',\n",
    "            '//div[contains(@class, \"article-content\")]',\n",
    "            '//div[@id=\"content\"]',\n",
    "            '//section[contains(@class, \"content\")]',\n",
    "            '//div[@class=\"post-content\"]'\n",
    "        ]\n",
    "\n",
    "        raw_html = \"\"\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                elem = driver.find_element(By.XPATH, selector)\n",
    "                raw_html = elem.get_attribute('innerHTML')\n",
    "                logging.info(f\"âœ… Extracted using XPath: {selector}\")\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # If no specific selector worked, fallback to full page source\n",
    "        if not raw_html:\n",
    "            logging.info(\"âš ï¸  XPath selectors failed. Using full page source.\")\n",
    "            raw_html = driver.page_source\n",
    "\n",
    "        # Clean content\n",
    "        text = clean_article_content(raw_html)\n",
    "        if text:\n",
    "            return text\n",
    "\n",
    "        # Final fallback: newspaper3k\n",
    "        logging.info(\"âš ï¸  Falling back to newspaper3k for: \" + url)\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text.strip() if article.text else None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"âŒ Exception extracting from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Sanitize filename\n",
    "def clean_filename(title):\n",
    "    return ''.join(c if c.isalnum() or c in (' ', '_') else '_' for c in title).replace(' ', '_')[:80] + \".txt\"\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Fetching article metadata...\")\n",
    "    feed = feedparser.parse(RSS_FEED)\n",
    "    entries = feed.entries[:50]  # Limit to top 50\n",
    "    logging.info(f\"Found {len(entries)} articles. Starting content extraction...\")\n",
    "\n",
    "    failed_articles = []\n",
    "    driver = get_driver()\n",
    "\n",
    "    for entry in tqdm(entries):\n",
    "        title = entry.title\n",
    "        link = entry.link\n",
    "        published = entry.published\n",
    "        source = entry.source.title if 'source' in entry else 'Unknown'\n",
    "\n",
    "        logging.info(f\"ðŸ“„ Processing: {title}\")\n",
    "\n",
    "        content = extract_article_content(driver, link)\n",
    "        if not content:\n",
    "            logging.info(f\"ðŸš« Could not extract content: {title}\")\n",
    "            failed_articles.append(link)\n",
    "            continue\n",
    "\n",
    "        filename = clean_filename(title)\n",
    "        filepath = os.path.join(SAVE_DIR, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Title: {title}\\nPublished: {published}\\nSource: {source}\\n\\n\")\n",
    "            f.write(content)\n",
    "\n",
    "        logging.info(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    if failed_articles:\n",
    "        logging.info(\"\\nðŸ§¾ Failed Articles:\")\n",
    "        for bad_url in failed_articles:\n",
    "            logging.info(\" - \" + bad_url)\n",
    "    else:\n",
    "        logging.info(\"âœ… All articles processed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a75fb",
   "metadata": {},
   "source": [
    "### NLP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acceccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.6 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4748df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLP analysis complete! Results saved to article_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load spaCy model\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load articles\n",
    "def load_articles(folder_path='articles'):\n",
    "    articles, filenames = [], []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                articles.append(content)\n",
    "                filenames.append(filename)\n",
    "    df = pd.DataFrame({'filename': filenames, 'article': articles})\n",
    "    df['cleaned_article'] = df['article'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    sentiment = 'Positive' if polarity > 0 else 'Negative' if polarity < 0 else 'Neutral'\n",
    "    return sentiment, polarity, subjectivity\n",
    "\n",
    "# Named Entity Recognition\n",
    "def get_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return ', '.join([f\"{ent.text} ({ent.label_})\" for ent in doc.ents])\n",
    "\n",
    "# Topic modeling (on all articles)\n",
    "def get_topics(texts, num_topics=3):\n",
    "    tokenized = [t.split() for t in texts]\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "# Process data\n",
    "df = load_articles('../articles')\n",
    "df[['sentiment', 'polarity', 'subjectivity']] = df['article'].apply(\n",
    "    lambda x: pd.Series(get_sentiment(x))\n",
    ")\n",
    "df['named_entities'] = df['article'].apply(get_named_entities)\n",
    "\n",
    "# Run topic modeling on all articles\n",
    "topics = get_topics(df['cleaned_article'].tolist())\n",
    "topic_summary = '; '.join([f\"Topic {idx +1}: {topic}\" for idx, topic in topics])\n",
    "\n",
    "# Save topic summary as same for all rows (you can improve to assign specific topic per article later)\n",
    "df['topics'] = topic_summary\n",
    "\n",
    "# Save results to CSV\n",
    "df.to_csv('../articles/article_analysis.csv', index=False)\n",
    "\n",
    "print(\"âœ… NLP analysis complete! Results saved to article_analysis.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca6c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fresh punkt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fresh punkt_tab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize works! âœ… Output: ['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Get nltk data base path\n",
    "nltk_data_base = os.path.join(os.path.expanduser('~'), 'AppData', 'Roaming', 'nltk_data')\n",
    "\n",
    "# Delete punkt and punkt_tab folders if they exist\n",
    "for subfolder in ['tokenizers/punkt', 'tokenizers/punkt_tab']:\n",
    "    folder_path = os.path.join(nltk_data_base, subfolder)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"Found existing {subfolder} folder at {folder_path}, deleting it...\")\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Deleted old {subfolder} folder.\")\n",
    "\n",
    "# Download punkt and punkt_tab cleanly\n",
    "print(\"Downloading fresh punkt...\")\n",
    "nltk.download('punkt')\n",
    "print(\"Downloading fresh punkt_tab...\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Test word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    tokens = word_tokenize(\"This is a test sentence.\")\n",
    "    print(\"word_tokenize works! âœ… Output:\", tokens)\n",
    "except Exception as e:\n",
    "    print(\"âŒ word_tokenize failed with error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f173efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and saved to ../news/climate_news_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_sentiment_analyzer():\n",
    "    \"\"\"Initialize VADER Sentiment Analyzer.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer\n",
    "\n",
    "def analyze_sentiment(text, analyzer):\n",
    "    \"\"\"Analyze sentiment polarity scores of a given text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score\n",
    "\n",
    "def assign_sentiment_label(score):\n",
    "    \"\"\"Assign Positive, Negative, Neutral labels based on compound score.\"\"\"\n",
    "    if score is None:\n",
    "        return \"Unknown\"\n",
    "    compound = score['compound']\n",
    "    if compound >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def apply_sentiment_analysis(input_csv=\"../news/climate_news_articles.csv\", output_csv=\"../news/climate_news_with_sentiment.csv\"):\n",
    "    \"\"\"Apply sentiment analysis on the news articles CSV and save results.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    analyzer = initialize_sentiment_analyzer()\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    df['sentiment_score'] = df['description'].apply(lambda x: analyze_sentiment(x, analyzer))\n",
    "    df['sentiment_label'] = df['sentiment_score'].apply(assign_sentiment_label)\n",
    "    \n",
    "    # Save the new dataframe\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Sentiment analysis completed and saved to {output_csv}\")\n",
    "\n",
    "# ðŸ›  Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_sentiment_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e0a1d",
   "metadata": {},
   "source": [
    "### NER and Extracting location and name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798ed1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292a20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition completed and saved to ../news/climate_news_with_ner.csv\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_spacy_model(model_name='en_core_web_sm'):\n",
    "    \"\"\"Initialize the spaCy NLP model.\"\"\"\n",
    "    nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "def extract_named_entities(text, nlp_model):\n",
    "    \"\"\"Extract named entities like location, organization, event from the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp_model(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def filter_entities_by_type(entities, types=[\"GPE\", \"LOC\", \"ORG\", \"EVENT\"]):\n",
    "    \"\"\"Filter entities based on specified types (like Location, Organization, Event).\"\"\"\n",
    "    return [ent for ent in entities if ent[1] in types]\n",
    "\n",
    "def apply_ner(input_csv=\"../news/climate_news_with_sentiment.csv\", output_csv=\"../news/climate_news_with_ner.csv\"):\n",
    "    \"\"\"Apply NER on news articles and save the enriched file.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    nlp_model = initialize_spacy_model()\n",
    "\n",
    "    df['named_entities'] = df['description'].apply(lambda x: filter_entities_by_type(extract_named_entities(x, nlp_model)))\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Named Entity Recognition completed and saved to {output_csv}\")\n",
    "\n",
    "# ðŸ›  Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_ner()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f60ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c35b2",
   "metadata": {},
   "source": [
    "### Topic Modelig system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b088ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Topics:\n",
      "Topic 0: 0.055*\"climate\" + 0.042*\"change\" + 0.015*\"new\" + 0.015*\"pope\" + 0.015*\"francis\"\n",
      "Topic 1: 0.064*\"climate\" + 0.051*\"change\" + 0.014*\"un\" + 0.014*\"action\" + 0.014*\"leaders\"\n",
      "Topic 2: 0.083*\"climate\" + 0.062*\"change\" + 0.032*\"news\" + 0.011*\"us\" + 0.011*\"making\"\n",
      "Topic 3: 0.066*\"climate\" + 0.052*\"change\" + 0.037*\"arizona\" + 0.030*\"republic\" + 0.016*\"environment\"\n",
      "Topic 4: 0.060*\"climate\" + 0.047*\"change\" + 0.014*\"global\" + 0.014*\"new\" + 0.014*\"francis\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "nltk.data.path.append(r\"C:\\Users\\SHYAM PANDIT\\AppData\\Roaming\\nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text for topic modeling without nltk punkt.\"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionary_and_corpus(texts):\n",
    "    \"\"\"Create dictionary and corpus for LDA model.\"\"\"\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def build_lda_model(corpus, dictionary, num_topics=5):\n",
    "    \"\"\"Build and return the LDA model.\"\"\"\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=15)\n",
    "    return lda_model\n",
    "\n",
    "def print_topics(lda_model, num_words=5):\n",
    "    \"\"\"Print the topics discovered by the LDA model.\"\"\"\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    for idx, topic in topics:\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "def apply_topic_modeling(input_csv=\"../news/climate_news_with_ner.csv\", text_column=\"description\", num_topics=5):\n",
    "    \"\"\"Apply topic modeling on news articles.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Check for NaN or empty values in the text column\n",
    "    if df[text_column].isnull().any():\n",
    "        print(f\"Warning: Some rows in '{text_column}' column have NaN values.\")\n",
    "        df = df.dropna(subset=[text_column])  # Remove rows with NaN values\n",
    "    \n",
    "    texts = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    dictionary, corpus = create_dictionary_and_corpus(texts)\n",
    "    lda_model = build_lda_model(corpus, dictionary, num_topics=num_topics)\n",
    "    \n",
    "    print(\"Generated Topics:\")\n",
    "    print_topics(lda_model)\n",
    "    \n",
    "    return lda_model, dictionary, corpus\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Apply topic modeling on the enriched CSV\n",
    "  lda_model, dictionary, corpus = apply_topic_modeling()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8b2de",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SHYAM PANDIT\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization completed and saved to ../news/climate_news_with_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def initialize_summarizer(model_name=\"facebook/bart-large-cnn\"):\n",
    "    \"\"\"Initialize Huggingface summarization pipeline.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "def summarize_text(text, summarizer, min_length=30, max_length=120):\n",
    "    \"\"\"Summarize a given text.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.split()) < 30:\n",
    "        return text  # Skip very short texts\n",
    "    \n",
    "    summary = summarizer(text, min_length=min_length, max_length=max_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def apply_text_summarization(input_csv=\"../news/climate_news_with_ner.csv\", output_csv=\"../news/climate_news_with_summary.csv\", text_column=\"description\"):\n",
    "    \"\"\"Apply summarization on news articles and save.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    summarizer = initialize_summarizer()\n",
    "\n",
    "    df['summary'] = df[text_column].apply(lambda x: summarize_text(x, summarizer))\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Summarization completed and saved to {output_csv}\")\n",
    "\n",
    "# ðŸ›  Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_text_summarization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4f39e",
   "metadata": {},
   "source": [
    "### Multilingual Nepali Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3045d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gnews\n",
      "  Using cached gnews-0.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting feedparser~=6.0.2 (from gnews)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting beautifulsoup4<5,>=4.9.3 (from gnews)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dnspython (from gnews)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from gnews) (2.32.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.9.3->gnews)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from beautifulsoup4<5,>=4.9.3->gnews) (4.13.2)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (2025.4.26)\n",
      "Using cached gnews-0.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6103 sha256=f042589a0b85700940df47575bbaf401de4559f957b432eb8e8091c248c4183e\n",
      "  Stored in directory: c:\\users\\shyam pandit\\appdata\\local\\pip\\cache\\wheels\\f0\\69\\93\\a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, soupsieve, feedparser, dnspython, beautifulsoup4, gnews\n",
      "\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   ---------------------------------------- 6/6 [gnews]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 dnspython-2.7.0 feedparser-6.0.11 gnews-0.4.1 sgmllib3k-1.0.0 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7641c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 Nepali articles to nepali_climate_news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "from gnews import GNews\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_news_client_nepali(language='ne', country='NP', max_results=50):\n",
    "    \"\"\"Initialize GNews client for Nepali language.\"\"\"\n",
    "    news = GNews(language=language, country=country, max_results=max_results)\n",
    "    return news\n",
    "\n",
    "def search_nepali_climate_news(news_client, keyword=\"à¤œà¤²à¤µà¤¾à¤¯à¥ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨\"):\n",
    "    \"\"\"Search Nepali climate-related news articles.\"\"\"\n",
    "    articles = news_client.get_news(keyword)\n",
    "    return articles\n",
    "\n",
    "def extract_article_info_nepali(articles):\n",
    "    \"\"\"Extract useful fields from Nepali news articles.\"\"\"\n",
    "    extracted_data = []\n",
    "    for article in articles:\n",
    "        extracted_data.append({\n",
    "            'title': article.get('title', ''),\n",
    "            'description': article.get('description', ''),\n",
    "            'published_date': article.get('published date', ''),\n",
    "            'url': article.get('url', ''),\n",
    "            'publisher': article.get('publisher', {}).get('title', '')\n",
    "        })\n",
    "    return extracted_data\n",
    "\n",
    "def save_nepali_articles_to_csv(extracted_data, filename=\"../news/nepali_climate_news_articles.csv\"):\n",
    "    \"\"\"Save extracted Nepali articles into a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')  # Save properly in Nepali\n",
    "    print(f\"Saved {len(df)} Nepali articles to {filename}\")\n",
    "\n",
    "# ðŸ›  Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    news_client = initialize_news_client_nepali(language='ne', country='NP', max_results=50)\n",
    "    articles = search_nepali_climate_news(news_client, keyword=\"à¤œà¤²à¤µà¤¾à¤¯à¥ à¤ªà¤°à¤¿à¤µà¤°à¥à¤¤à¤¨\")  # \"Climate Change\" in Nepali\n",
    "    extracted_data = extract_article_info_nepali(articles)\n",
    "    save_nepali_articles_to_csv(extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3620183b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ðŸ›  Example Usage\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mapply_multilingual_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 37\u001b[0m, in \u001b[0;36mapply_multilingual_processing\u001b[1;34m(input_csv, output_csv, text_column)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Full pipeline: Translate Nepali text â†’ Summarize â†’ Save.\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_csv)\n\u001b[1;32m---> 37\u001b[0m nepali_tokenizer, nepali_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_translation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m initialize_multilingual_summarizer()\n\u001b[0;32m     40\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_translation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[text_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: translate_nepali_to_english(x, nepali_tokenizer, nepali_model))\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mload_translation_model\u001b[1;34m(src_lang, tgt_lang)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load translation model for Nepali to English.\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1826\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, pipeline\n",
    "\n",
    "def load_translation_model(src_lang=\"en\", tgt_lang=\"np\"):\n",
    "    \"\"\"Load translation model for Nepali to English.\"\"\"\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def translate_nepali_to_english(text, tokenizer, model):\n",
    "    \"\"\"Translate Nepali text to English.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def initialize_multilingual_summarizer(model_name=\"facebook/bart-large-cnn\"):\n",
    "    \"\"\"Initialize a summarizer model (already in English).\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "def summarize_nepali_text(text, nepali_tokenizer, nepali_model, summarizer, min_length=30, max_length=120):\n",
    "    \"\"\"Translate Nepali â†’ English â†’ Summarize.\"\"\"\n",
    "    english_text = translate_nepali_to_english(text, nepali_tokenizer, nepali_model)\n",
    "    if len(english_text.split()) < 30:\n",
    "        return english_text  \n",
    "\n",
    "    summary = summarizer(english_text, min_length=min_length, max_length=max_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def apply_multilingual_processing(input_csv=\"../news/nepali_climate_news_articles.csv\", output_csv=\"../news/nepali_climate_news_summary.csv\", text_column=\"description\"):\n",
    "    \"\"\"Full pipeline: Translate Nepali text â†’ Summarize â†’ Save.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    nepali_tokenizer, nepali_model = load_translation_model()\n",
    "    summarizer = initialize_multilingual_summarizer()\n",
    "\n",
    "    df['english_translation'] = df[text_column].apply(lambda x: translate_nepali_to_english(x, nepali_tokenizer, nepali_model))\n",
    "    df['summary'] = df['english_translation'].apply(lambda x: summarizer(x, min_length=30, max_length=120, do_sample=False)[0]['summary_text'] if len(x.split()) > 30 else x)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Multilingual processing completed and saved to {output_csv}\")\n",
    "\n",
    "# ðŸ›  Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    apply_multilingual_processing()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
