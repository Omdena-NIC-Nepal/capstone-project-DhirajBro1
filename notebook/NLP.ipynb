{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df36318c",
   "metadata": {},
   "source": [
    "### Collecting News,article on Climate Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed69a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 articles to climate_news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "from gnews import GNews\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_news_client(language='en', country='NP', max_results=50):\n",
    "    \"\"\"Initialize GNews client with language, country, and max results.\"\"\"\n",
    "    news = GNews(language=language, country=country, max_results=max_results)\n",
    "    return news\n",
    "\n",
    "def search_news_articles(news_client, keyword):\n",
    "    \"\"\"Search news articles related to a keyword.\"\"\"\n",
    "    articles = news_client.get_news(keyword)\n",
    "    return articles\n",
    "\n",
    "def extract_article_info(articles):\n",
    "    \"\"\"Extract useful fields from raw article data.\"\"\"\n",
    "    extracted_data = []\n",
    "    for article in articles:\n",
    "        extracted_data.append({\n",
    "            'title': article.get('title', ''),\n",
    "            'description': article.get('description', ''),\n",
    "            'published_date': article.get('published date', ''),\n",
    "            'url': article.get('url', ''),\n",
    "            'publisher': article.get('publisher', {}).get('title', '')\n",
    "        })\n",
    "    return extracted_data\n",
    "\n",
    "def save_articles_to_csv(extracted_data, filename=\"climate_news_articles.csv\"):\n",
    "    \"\"\"Save extracted articles into a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(df)} articles to {filename}\")\n",
    "\n",
    "# 🛠 Example Usage:\n",
    "if __name__ == \"__main__\":pip install nltk\n",
    "\n",
    "    news_client = initialize_news_client(language='en', country='NP', max_results=50)\n",
    "    articles = search_news_articles(news_client, keyword=\"climate change\")\n",
    "    extracted_data = extract_article_info(articles)\n",
    "    save_articles_to_csv(extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a75fb",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f173efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed and saved to ../news/climate_news_with_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_sentiment_analyzer():\n",
    "    \"\"\"Initialize VADER Sentiment Analyzer.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer\n",
    "\n",
    "def analyze_sentiment(text, analyzer):\n",
    "    \"\"\"Analyze sentiment polarity scores of a given text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score\n",
    "\n",
    "def assign_sentiment_label(score):\n",
    "    \"\"\"Assign Positive, Negative, Neutral labels based on compound score.\"\"\"\n",
    "    if score is None:\n",
    "        return \"Unknown\"\n",
    "    compound = score['compound']\n",
    "    if compound >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def apply_sentiment_analysis(input_csv=\"../news/climate_news_articles.csv\", output_csv=\"../news/climate_news_with_sentiment.csv\"):\n",
    "    \"\"\"Apply sentiment analysis on the news articles CSV and save results.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    analyzer = initialize_sentiment_analyzer()\n",
    "    \n",
    "    # Analyze sentiment\n",
    "    df['sentiment_score'] = df['description'].apply(lambda x: analyze_sentiment(x, analyzer))\n",
    "    df['sentiment_label'] = df['sentiment_score'].apply(assign_sentiment_label)\n",
    "    \n",
    "    # Save the new dataframe\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Sentiment analysis completed and saved to {output_csv}\")\n",
    "\n",
    "# 🛠 Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_sentiment_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e0a1d",
   "metadata": {},
   "source": [
    "### NER and Extracting location and name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798ed1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.1/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 4.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "292a20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition completed and saved to ../news/climate_news_with_ner.csv\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_spacy_model(model_name='en_core_web_sm'):\n",
    "    \"\"\"Initialize the spaCy NLP model.\"\"\"\n",
    "    nlp = spacy.load(model_name)\n",
    "    return nlp\n",
    "\n",
    "def extract_named_entities(text, nlp_model):\n",
    "    \"\"\"Extract named entities like location, organization, event from the text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    doc = nlp_model(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def filter_entities_by_type(entities, types=[\"GPE\", \"LOC\", \"ORG\", \"EVENT\"]):\n",
    "    \"\"\"Filter entities based on specified types (like Location, Organization, Event).\"\"\"\n",
    "    return [ent for ent in entities if ent[1] in types]\n",
    "\n",
    "def apply_ner(input_csv=\"../news/climate_news_with_sentiment.csv\", output_csv=\"../news/climate_news_with_ner.csv\"):\n",
    "    \"\"\"Apply NER on news articles and save the enriched file.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    nlp_model = initialize_spacy_model()\n",
    "\n",
    "    df['named_entities'] = df['description'].apply(lambda x: filter_entities_by_type(extract_named_entities(x, nlp_model)))\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Named Entity Recognition completed and saved to {output_csv}\")\n",
    "\n",
    "# 🛠 Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_ner()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f60ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SHYAM\n",
      "[nltk_data]     PANDIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c35b2",
   "metadata": {},
   "source": [
    "### Topic Modelig system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b088ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Topics:\n",
      "Topic 0: 0.055*\"climate\" + 0.042*\"change\" + 0.015*\"new\" + 0.015*\"pope\" + 0.015*\"francis\"\n",
      "Topic 1: 0.064*\"climate\" + 0.051*\"change\" + 0.014*\"un\" + 0.014*\"action\" + 0.014*\"leaders\"\n",
      "Topic 2: 0.083*\"climate\" + 0.062*\"change\" + 0.032*\"news\" + 0.011*\"us\" + 0.011*\"making\"\n",
      "Topic 3: 0.066*\"climate\" + 0.052*\"change\" + 0.037*\"arizona\" + 0.030*\"republic\" + 0.016*\"environment\"\n",
      "Topic 4: 0.060*\"climate\" + 0.047*\"change\" + 0.014*\"global\" + 0.014*\"new\" + 0.014*\"francis\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "nltk.data.path.append(r\"C:\\Users\\SHYAM PANDIT\\AppData\\Roaming\\nltk_data\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text for topic modeling without nltk punkt.\"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dictionary_and_corpus(texts):\n",
    "    \"\"\"Create dictionary and corpus for LDA model.\"\"\"\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def build_lda_model(corpus, dictionary, num_topics=5):\n",
    "    \"\"\"Build and return the LDA model.\"\"\"\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=15)\n",
    "    return lda_model\n",
    "\n",
    "def print_topics(lda_model, num_words=5):\n",
    "    \"\"\"Print the topics discovered by the LDA model.\"\"\"\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    for idx, topic in topics:\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "def apply_topic_modeling(input_csv=\"../news/climate_news_with_ner.csv\", text_column=\"description\", num_topics=5):\n",
    "    \"\"\"Apply topic modeling on news articles.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Check for NaN or empty values in the text column\n",
    "    if df[text_column].isnull().any():\n",
    "        print(f\"Warning: Some rows in '{text_column}' column have NaN values.\")\n",
    "        df = df.dropna(subset=[text_column])  # Remove rows with NaN values\n",
    "    \n",
    "    texts = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    dictionary, corpus = create_dictionary_and_corpus(texts)\n",
    "    lda_model = build_lda_model(corpus, dictionary, num_topics=num_topics)\n",
    "    \n",
    "    print(\"Generated Topics:\")\n",
    "    print_topics(lda_model)\n",
    "    \n",
    "    return lda_model, dictionary, corpus\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Apply topic modeling on the enriched CSV\n",
    "  lda_model, dictionary, corpus = apply_topic_modeling()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8b2de",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ee85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SHYAM PANDIT\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization completed and saved to ../news/climate_news_with_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "def initialize_summarizer(model_name=\"facebook/bart-large-cnn\"):\n",
    "    \"\"\"Initialize Huggingface summarization pipeline.\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "def summarize_text(text, summarizer, min_length=30, max_length=120):\n",
    "    \"\"\"Summarize a given text.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.split()) < 30:\n",
    "        return text  # Skip very short texts\n",
    "    \n",
    "    summary = summarizer(text, min_length=min_length, max_length=max_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def apply_text_summarization(input_csv=\"../news/climate_news_with_ner.csv\", output_csv=\"../news/climate_news_with_summary.csv\", text_column=\"description\"):\n",
    "    \"\"\"Apply summarization on news articles and save.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    summarizer = initialize_summarizer()\n",
    "\n",
    "    df['summary'] = df[text_column].apply(lambda x: summarize_text(x, summarizer))\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Summarization completed and saved to {output_csv}\")\n",
    "\n",
    "# 🛠 Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    apply_text_summarization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4f39e",
   "metadata": {},
   "source": [
    "### Multilingual Nepali Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3045d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gnews\n",
      "  Using cached gnews-0.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting feedparser~=6.0.2 (from gnews)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting beautifulsoup4<5,>=4.9.3 (from gnews)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dnspython (from gnews)\n",
      "  Using cached dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from gnews) (2.32.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.9.3->gnews)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from beautifulsoup4<5,>=4.9.3->gnews) (4.13.2)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shyam pandit\\omdena assignment\\assignment 7\\.venv\\lib\\site-packages (from requests->gnews) (2025.4.26)\n",
      "Using cached gnews-0.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6103 sha256=f042589a0b85700940df47575bbaf401de4559f957b432eb8e8091c248c4183e\n",
      "  Stored in directory: c:\\users\\shyam pandit\\appdata\\local\\pip\\cache\\wheels\\f0\\69\\93\\a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, soupsieve, feedparser, dnspython, beautifulsoup4, gnews\n",
      "\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ---------------------------------------- 0/6 [sgmllib3k]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------ --------------------------------- 1/6 [soupsieve]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   ------------- -------------------------- 2/6 [feedparser]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------- ------------------- 3/6 [dnspython]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   -------------------------- ------------- 4/6 [beautifulsoup4]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   --------------------------------- ------ 5/6 [gnews]\n",
      "   ---------------------------------------- 6/6 [gnews]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 dnspython-2.7.0 feedparser-6.0.11 gnews-0.4.1 sgmllib3k-1.0.0 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7641c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 Nepali articles to nepali_climate_news_articles.csv\n"
     ]
    }
   ],
   "source": [
    "from gnews import GNews\n",
    "import pandas as pd\n",
    "\n",
    "def initialize_news_client_nepali(language='ne', country='NP', max_results=50):\n",
    "    \"\"\"Initialize GNews client for Nepali language.\"\"\"\n",
    "    news = GNews(language=language, country=country, max_results=max_results)\n",
    "    return news\n",
    "\n",
    "def search_nepali_climate_news(news_client, keyword=\"जलवायु परिवर्तन\"):\n",
    "    \"\"\"Search Nepali climate-related news articles.\"\"\"\n",
    "    articles = news_client.get_news(keyword)\n",
    "    return articles\n",
    "\n",
    "def extract_article_info_nepali(articles):\n",
    "    \"\"\"Extract useful fields from Nepali news articles.\"\"\"\n",
    "    extracted_data = []\n",
    "    for article in articles:\n",
    "        extracted_data.append({\n",
    "            'title': article.get('title', ''),\n",
    "            'description': article.get('description', ''),\n",
    "            'published_date': article.get('published date', ''),\n",
    "            'url': article.get('url', ''),\n",
    "            'publisher': article.get('publisher', {}).get('title', '')\n",
    "        })\n",
    "    return extracted_data\n",
    "\n",
    "def save_nepali_articles_to_csv(extracted_data, filename=\"../news/nepali_climate_news_articles.csv\"):\n",
    "    \"\"\"Save extracted Nepali articles into a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')  # Save properly in Nepali\n",
    "    print(f\"Saved {len(df)} Nepali articles to {filename}\")\n",
    "\n",
    "# 🛠 Example Usage:\n",
    "if __name__ == \"__main__\":\n",
    "    news_client = initialize_news_client_nepali(language='ne', country='NP', max_results=50)\n",
    "    articles = search_nepali_climate_news(news_client, keyword=\"जलवायु परिवर्तन\")  # \"Climate Change\" in Nepali\n",
    "    extracted_data = extract_article_info_nepali(articles)\n",
    "    save_nepali_articles_to_csv(extracted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3620183b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 🛠 Example Usage\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mapply_multilingual_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 37\u001b[0m, in \u001b[0;36mapply_multilingual_processing\u001b[1;34m(input_csv, output_csv, text_column)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Full pipeline: Translate Nepali text → Summarize → Save.\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_csv)\n\u001b[1;32m---> 37\u001b[0m nepali_tokenizer, nepali_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_translation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m initialize_multilingual_summarizer()\n\u001b[0;32m     40\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish_translation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[text_column]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: translate_nepali_to_english(x, nepali_tokenizer, nepali_model))\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36mload_translation_model\u001b[1;34m(src_lang, tgt_lang)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load translation model for Nepali to English.\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_lang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer, model\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\Omdena Assignment\\assignment 7\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1826\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, pipeline\n",
    "\n",
    "def load_translation_model(src_lang=\"en\", tgt_lang=\"np\"):\n",
    "    \"\"\"Load translation model for Nepali to English.\"\"\"\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def translate_nepali_to_english(text, tokenizer, model):\n",
    "    \"\"\"Translate Nepali text to English.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return \"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "def initialize_multilingual_summarizer(model_name=\"facebook/bart-large-cnn\"):\n",
    "    \"\"\"Initialize a summarizer model (already in English).\"\"\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    return summarizer\n",
    "\n",
    "def summarize_nepali_text(text, nepali_tokenizer, nepali_model, summarizer, min_length=30, max_length=120):\n",
    "    \"\"\"Translate Nepali → English → Summarize.\"\"\"\n",
    "    english_text = translate_nepali_to_english(text, nepali_tokenizer, nepali_model)\n",
    "    if len(english_text.split()) < 30:\n",
    "        return english_text  \n",
    "\n",
    "    summary = summarizer(english_text, min_length=min_length, max_length=max_length, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "def apply_multilingual_processing(input_csv=\"../news/nepali_climate_news_articles.csv\", output_csv=\"../news/nepali_climate_news_summary.csv\", text_column=\"description\"):\n",
    "    \"\"\"Full pipeline: Translate Nepali text → Summarize → Save.\"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    nepali_tokenizer, nepali_model = load_translation_model()\n",
    "    summarizer = initialize_multilingual_summarizer()\n",
    "\n",
    "    df['english_translation'] = df[text_column].apply(lambda x: translate_nepali_to_english(x, nepali_tokenizer, nepali_model))\n",
    "    df['summary'] = df['english_translation'].apply(lambda x: summarizer(x, min_length=30, max_length=120, do_sample=False)[0]['summary_text'] if len(x.split()) > 30 else x)\n",
    "\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Multilingual processing completed and saved to {output_csv}\")\n",
    "\n",
    "# 🛠 Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    apply_multilingual_processing()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
