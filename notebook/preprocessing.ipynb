{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe07734",
   "metadata": {},
   "source": [
    "### Preprocessing Extreme Weather events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71763e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../raw_data/Weather_and_Climate_data/extreme_weather_events.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../raw_data/Weather_and_Climate_data/extreme_weather_events.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())         \u001b[38;5;66;03m# Show top 5 rows\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../raw_data/Weather_and_Climate_data/extreme_weather_events.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../raw_data/Weather_and_Climate_data/extreme_weather_events.csv\", engine=\"python\")\n",
    "print(df.head())         # Show top 5 rows\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c1a6e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\295919590.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Clean column names (remove extra spaces, unify case)\n",
    "df.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.replace(\"(\", \"\").str.replace(\")\", \"\").str.replace(\"'\", \"\").str.lower()\n",
    "#remove duplicates and trim strings\n",
    "df.drop_duplicates(inplace=True)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "c:\\Users\\SHYAM PANDIT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9236\\3385968364.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.replace(['', '..', 'N/A', 'na'], np.nan, inplace=True)\n",
    "numeric_columns = [\n",
    "    'magnitude', 'latitude', 'longitude', 'total_deaths', 'no._injured',\n",
    "    'no._affected', 'no._homeless', 'total_affected',\n",
    "    'reconstruction_costs_000_us$', 'insured_damage_000_us$',\n",
    "    'total_damage_000_us$', 'cpi'\n",
    "]\n",
    "for col in numeric_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Option 1: Fill with median (common for skewed disaster data)\n",
    "for col in numeric_columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4650a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "cost_columns = ['reconstruction_costs_000_us$', 'insured_damage_000_us$', 'total_damage_000_us$']\n",
    "for col in cost_columns:\n",
    "    df[col] = df[col] * 1000  # converting to actual USD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "676b361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to integers after filling NaNs\n",
    "df['start_day'] = df['start_day'].fillna(1).astype(int)\n",
    "df['start_month'] = df['start_month'].fillna(1).astype(int)\n",
    "df['start_year'] = df['start_year'].fillna(1).astype(int)\n",
    "\n",
    "df['end_day'] = df['end_day'].fillna(1).astype(int)\n",
    "df['end_month'] = df['end_month'].fillna(1).astype(int)\n",
    "df['end_year'] = df['end_year'].fillna(1).astype(int)\n",
    "\n",
    "# Safely create datetime columns using apply\n",
    "df['start_date'] = pd.to_datetime(df.apply(lambda row: f\"{int(row['start_year'])}-{int(row['start_month'])}-{int(row['start_day'])}\", axis=1), errors='coerce')\n",
    "df['end_date'] = pd.to_datetime(df.apply(lambda row: f\"{int(row['end_year'])}-{int(row['end_month'])}-{int(row['end_day'])}\", axis=1), errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3dd4c5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Temporal Alignment\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Temporal Alignment\n",
    "df.sort_values(by='start_date', inplace=True)\n",
    "df.set_index('start_date', inplace=True)\n",
    "df['start_date'] = pd.to_datetime(df['start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6060e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Georeferencing for Spatial Analysis\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "df = df.dropna(subset=['latitude', 'longitude'])  # ensure we have valid coordinates\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28eb6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df.reset_index(inplace=True)\n",
    "df.to_csv(\"../processed_data/processed_extreme_weather_events.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3024587c",
   "metadata": {},
   "source": [
    "### Preprocessing Glacial Lake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e7ef1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(\"../raw_data/Enviromental_data/Glacial_lake_data/nepal_glacial_lakes_2001/data/NepalGlacialLake2001.shp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f67d6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9556\\224108815.py:59: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = all_years.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid geometries: 0\n",
      "Missing Gl_Code: 0\n",
      "Negative Areas: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_9556\\224108815.py:70: UserWarning: Geometry column does not contain geometry.\n",
      "  all_years['geometry'] = all_years['geometry'].apply(lambda geom: geom.wkt)\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import os\n",
    "\n",
    "# STEP 1: Load Data\n",
    "gdf_2001 = gpd.read_file(\"../raw_data/Enviromental_data/Glacial_lake_data/nepal_glacial_lakes_2001/data/NepalGlacialLake2001.shp\")\n",
    "gdf_2011 = gpd.read_file(\"../raw_data/Enviromental_data/Glacial_lake_data/nepal_glacial_lakes_2011/data/NepalGlacialLake2011.shp\")\n",
    "gdf_2015 = gpd.read_file(\"../raw_data/Enviromental_data/Glacial_lake_data/data/GL_3basins_2015.shp\")\n",
    "\n",
    "# STEP 2: Clean Column Names\n",
    "def clean_columns(gdf):\n",
    "    gdf.columns = gdf.columns.str.strip().str.replace(\" \", \"_\").str.lower()\n",
    "    return gdf\n",
    "\n",
    "gdf_2001 = clean_columns(gdf_2001)\n",
    "gdf_2011 = clean_columns(gdf_2011)\n",
    "gdf_2015 = clean_columns(gdf_2015)\n",
    "\n",
    "# STEP 3: Handle Missing Values\n",
    "def fill_missing(gdf):\n",
    "    gdf['gl_name'] = gdf['gl_name'].fillna('Unknown')\n",
    "    gdf['gl_area'] = gdf['gl_area'].fillna(gdf['gl_area'].mean())\n",
    "    gdf['elevation'] = gdf['elevation'].fillna(gdf['elevation'].mean())\n",
    "    return gdf\n",
    "\n",
    "gdf_2001 = fill_missing(gdf_2001)\n",
    "gdf_2011 = fill_missing(gdf_2011)\n",
    "gdf_2015['gl_area'] = gdf_2015['area']  # normalize column names\n",
    "gdf_2015['elevation'] = gdf_2015['elevation']  # already in decimal\n",
    "\n",
    "# STEP 4: Normalize CRS and Units\n",
    "for gdf in [gdf_2001, gdf_2011, gdf_2015]:\n",
    "    gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# STEP 5: Add Year Column\n",
    "gdf_2001[\"year\"] = 2001\n",
    "gdf_2011[\"year\"] = 2011\n",
    "gdf_2015[\"year\"] = 2015\n",
    "\n",
    "# STEP 6: Select Common Columns\n",
    "cols = ['gl_code', 'gl_area', 'elevation', 'year', 'geometry']\n",
    "gdf_2001 = gdf_2001[cols]\n",
    "gdf_2011 = gdf_2011[cols]\n",
    "gdf_2015['gl_code'] = gdf_2015['gl_id']  # rename ID for consistency\n",
    "gdf_2015 = gdf_2015[cols]\n",
    "\n",
    "# STEP 7: Combine Datasets\n",
    "all_years = pd.concat([gdf_2001, gdf_2011, gdf_2015], ignore_index=True)\n",
    "\n",
    "# STEP 8: Data Lineage\n",
    "all_years['source'] = all_years['year'].map({\n",
    "    2001: 'glacial_lakes_2001.shp',\n",
    "    2011: 'glacial_lakes_2011.shp',\n",
    "    2015: 'glacial_lakes_2015.shp'\n",
    "})\n",
    "\n",
    "# STEP 9: Add Latitude and Longitude from centroid\n",
    "centroids = all_years.geometry.centroid\n",
    "all_years['Longitude'] = centroids.x\n",
    "all_years['Latitude'] = centroids.y\n",
    "\n",
    "# STEP 10: Validation Checks\n",
    "invalid_geoms = all_years[~all_years.geometry.is_valid]\n",
    "print(f\"Invalid geometries: {len(invalid_geoms)}\")\n",
    "print(f\"Missing Gl_Code: {all_years['gl_code'].isnull().sum()}\")\n",
    "print(f\"Negative Areas: {(all_years['gl_area'] < 0).sum()}\")\n",
    "\n",
    "# STEP 11: Export as CSV with WKT Geometry\n",
    "all_years['geometry'] = all_years['geometry'].apply(lambda geom: geom.wkt)\n",
    "all_years.to_csv(\"../processed_data/processed_glacial_lake_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8625a",
   "metadata": {},
   "source": [
    "### Preprocessing Land cover/Land Use data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe5d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2000)\\data\\lc2000.tif...\n",
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2010)\\data\\lc2010.tif...\n",
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2022)\\data\\lc2022.tif...\n",
      "Saved sampled CSV to ../processed_data/LULC_Combined.csv\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "LULC_YEARS = ['2000', '2010', '2022']\n",
    "INPUT_FOLDER = '../raw_data/Enviromental_data/LULC'\n",
    "OUTPUT_CSV = '../processed_data/LULC_Combined.csv'\n",
    "CLASS_MAPPING = {\n",
    "    1: 'Forest',\n",
    "    2: 'Shrubland',\n",
    "    3: 'Grassland',\n",
    "    4: 'Agriculture',\n",
    "    5: 'Built-up',\n",
    "    6: 'Waterbody',\n",
    "    7: 'Bareland'\n",
    "}\n",
    "SAMPLE_RATE = 20  # Sample every 20th pixel to reduce memory usage\n",
    "\n",
    "def extract_raster_to_csv_sampled(tif_path, year):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        band = src.read(1)\n",
    "        band = band[::SAMPLE_RATE, ::SAMPLE_RATE]  # Downsample\n",
    "        transform = src.transform\n",
    "\n",
    "        rows, cols = np.where((band != src.nodata) & (band != 0))\n",
    "        xs, ys = rasterio.transform.xy(transform, rows * SAMPLE_RATE, cols * SAMPLE_RATE)\n",
    "        values = band[rows, cols]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Year\": year,\n",
    "        \"Latitude\": ys,\n",
    "        \"Longitude\": xs,\n",
    "        \"LULC_Code\": values,\n",
    "        \"LULC_Label\": [CLASS_MAPPING.get(v, 'Unknown') for v in values],\n",
    "        \"Source\": \"ICIMOD\"\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in LULC_YEARS:\n",
    "        tif_file = os.path.join(INPUT_FOLDER, f'lulc({year})', 'data', f'lc{year}.tif')\n",
    "        print(f\"Processing {tif_file}...\")\n",
    "        df = extract_raster_to_csv_sampled(tif_file, year)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved sampled CSV to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10945d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2000)\\data\\lc2000.tif...\n",
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2010)\\data\\lc2010.tif...\n",
      "Processing ../raw_data/Enviromental_data/LULC\\lulc(2022)\\data\\lc2022.tif...\n",
      "Saved sampled CSV to ../processed_data/LULC_Sampled.csv\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "LULC_YEARS = ['2000', '2010', '2022']\n",
    "INPUT_FOLDER = '../raw_data/Enviromental_data/LULC'\n",
    "OUTPUT_CSV = '../processed_data/LULC_Sampled.csv'\n",
    "CLASS_MAPPING = {\n",
    "    1: 'Forest',\n",
    "    2: 'Shrubland',\n",
    "    3: 'Grassland',\n",
    "    4: 'Agriculture',\n",
    "    5: 'Built-up',\n",
    "    6: 'Waterbody',\n",
    "    7: 'Bareland'\n",
    "}\n",
    "SAMPLE_RATE = 20  # Sample every 20th pixel\n",
    "\n",
    "def validate_data(df):\n",
    "    return df[\n",
    "        (df[\"Latitude\"].between(-90, 90)) &\n",
    "        (df[\"Longitude\"].between(-180, 180)) &\n",
    "        (df[\"LULC_Code\"].isin(CLASS_MAPPING.keys()))\n",
    "    ]\n",
    "\n",
    "def extract_raster_to_csv_sampled(tif_path, year):\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        band = src.read(1)\n",
    "        band = band[::SAMPLE_RATE, ::SAMPLE_RATE]\n",
    "        transform = src.transform\n",
    "\n",
    "        rows, cols = np.where((band != src.nodata) & (band != 0))\n",
    "        xs, ys = rasterio.transform.xy(transform, rows * SAMPLE_RATE, cols * SAMPLE_RATE)\n",
    "        values = band[rows, cols]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Year\": year,\n",
    "        \"Latitude\": ys,\n",
    "        \"Longitude\": xs,\n",
    "        \"LULC_Code\": values,\n",
    "        \"LULC_Label\": [CLASS_MAPPING.get(v, 'Unknown') for v in values],\n",
    "        \"Source\": \"ICIMOD\"\n",
    "    })\n",
    "\n",
    "    df = validate_data(df)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    all_dfs = []\n",
    "\n",
    "    for year in LULC_YEARS:\n",
    "        tif_file = os.path.join(INPUT_FOLDER, f'lulc({year})', 'data', f'lc{year}.tif')\n",
    "        print(f\"Processing {tif_file}...\")\n",
    "        df = extract_raster_to_csv_sampled(tif_file, year)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved sampled CSV to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4506b2",
   "metadata": {},
   "source": [
    "### Preprocessing River Discharge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda1a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e3eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grdc_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin1') as file:\n",
    "            lines = file.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(filepath, 'r', encoding='ISO-8859-1') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "    metadata = {}\n",
    "    data_start = 0\n",
    "\n",
    "    # Extract metadata\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.startswith(\"# River:\"):\n",
    "            metadata['River'] = line.split(':', 1)[1].strip()\n",
    "        elif line.startswith(\"# Station:\"):\n",
    "            metadata['Station'] = line.split(':', 1)[1].strip()\n",
    "        elif line.startswith(\"# Latitude\"):\n",
    "            metadata['Latitude'] = float(line.split(':', 1)[1].strip())\n",
    "        elif line.startswith(\"# Longitude\"):\n",
    "            metadata['Longitude'] = float(line.split(':', 1)[1].strip())\n",
    "        elif line.startswith(\"# Next downstream station:\"):\n",
    "            metadata['NextStation'] = line.split(':', 1)[1].strip()\n",
    "        elif line.strip() == \"# DATA\":\n",
    "            data_start = idx + 2\n",
    "            break\n",
    "\n",
    "    # Read tabular data\n",
    "    data = pd.read_csv(\n",
    "        filepath,\n",
    "        sep=';',\n",
    "        skiprows=data_start,\n",
    "        names=['Date', 'Time', 'Original', 'Calculated', 'Flag'],\n",
    "        na_values=['-999.000'],\n",
    "        encoding='latin1'\n",
    "    )\n",
    "\n",
    "    # Use original value only\n",
    "    data['Discharge'] = data['Original']\n",
    "    data['Year'] = pd.to_datetime(data['Date'], errors='coerce').dt.year\n",
    "    data = data.dropna(subset=['Year'])\n",
    "\n",
    "    for key, val in metadata.items():\n",
    "        data[key] = val\n",
    "\n",
    "    return data[['River', 'Station', 'Latitude', 'Longitude', 'NextStation', 'Year', 'Discharge']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1a15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548350_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548450_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548460_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548500_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548550_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548610_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2548620_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2549235_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2549300_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2549350_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2549400_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2549500_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550200_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550225_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550300_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550350_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550400_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550450_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550500_Q_Day.Cmd.txt\n",
      "Skipped empty data in file: ../raw_data/Enviromental_data/river discharge data\\2550600_Q_Day.Cmd.txt\n"
     ]
    }
   ],
   "source": [
    "folder_path = '../raw_data/Enviromental_data/river discharge data'  # Replace with your actual path\n",
    "all_files = glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in all_files:\n",
    "    try:\n",
    "        river_data = parse_grdc_file(file)\n",
    "        if not river_data.empty:\n",
    "            combined_df = pd.concat([combined_df, river_data], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Skipped empty data in file: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {file}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing discharge using group mean (or interpolate)\n",
    "combined_df['Discharge'] = combined_df.groupby(['River', 'Station'])['Discharge'].transform(\n",
    "    lambda x: x.fillna(x.mean())\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65599ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined_df['Discharge_Normalized'] = scaler.fit_transform(combined_df[['Discharge']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ad33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df = combined_df.groupby(['River', 'Station', 'Latitude', 'Longitude', 'NextStation', 'Year']) \\\n",
    "                       .agg({'Discharge': 'mean', 'Discharge_Normalized': 'mean'}) \\\n",
    "                       .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46f1dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"Title\" : \"Annual Mean Discharge Data\",\n",
    "    \"River\": \"Name of the river\",\n",
    "    \"Station\": \"Measurement station name\",\n",
    "    \"Latitude\": \"Station latitude in decimal degrees\",\n",
    "    \"Longitude\": \"Station longitude in decimal degrees\",\n",
    "    \"NextStation\": \"Next downstream station ID\",\n",
    "    \"Year\": \"Year of observation\",\n",
    "    \"Discharge\": \"Annual mean discharge in m³/s\",\n",
    "    \"Discharge_Normalized\": \"Z-score normalized discharge\"\n",
    "}\n",
    "\n",
    "# Save schema documentation\n",
    "with open(\"processed_data_schema.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for col, desc in schema.items():\n",
    "        f.write(f\"{col}: {desc}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad34786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert annual_df.isnull().sum().sum() == 0, \"Missing values found!\"\n",
    "assert (annual_df['Year'] >= 1900).all(), \"Invalid year values\"\n",
    "assert annual_df['Discharge'].min() >= 0, \"Discharge cannot be negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac407fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot raw and normalized separately\n",
    "pivot_raw = combined_df.pivot_table(\n",
    "    index=['River', 'Station', 'Latitude', 'Longitude', 'NextStation'],\n",
    "    columns='Year',\n",
    "    values='Discharge'\n",
    ")\n",
    "\n",
    "pivot_norm = combined_df.pivot_table(\n",
    "    index=['River', 'Station', 'Latitude', 'Longitude', 'NextStation'],\n",
    "    columns='Year',\n",
    "    values='Discharge_Normalized'\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "pivot_raw.columns = [f\"Raw_{col}(m³/s)\" for col in pivot_raw.columns]\n",
    "pivot_norm.columns = [f\"Norm_{col}(m³/s)\" for col in pivot_norm.columns]\n",
    "\n",
    "# Combine\n",
    "pivot_both = pd.concat([pivot_raw, pivot_norm], axis=1).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final discharge data saved as 'final_river_discharge.csv'\n"
     ]
    }
   ],
   "source": [
    "# Sort columns: group raw first, then normalized\n",
    "raw_cols = sorted([col for col in pivot_both.columns if col.startswith(\"Raw_\")])\n",
    "norm_cols = sorted([col for col in pivot_both.columns if col.startswith(\"Norm_\")])\n",
    "main_cols = ['River', 'Station', 'Latitude', 'Longitude', 'NextStation']\n",
    "\n",
    "# Reorder the DataFrame\n",
    "final_df = pivot_both[main_cols + raw_cols + norm_cols]\n",
    "\n",
    "# Export with UTF-8 encoding to handle special characters like ³\n",
    "final_df.to_csv(\"../raw_data/Enviromental_data/river discharge data/final_river_discharge.csv\", index=False, encoding='utf-8')\n",
    "print(\" Final discharge data saved as 'final_river_discharge.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651531c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Interpolated data saved as 'final_river_discharge_interpolated.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named 'final_df'\n",
    "final_df = pd.read_csv(\"../raw_data/Enviromental_data/river discharge data/final_river_discharge.csv\")\n",
    "\n",
    "# Interpolate missing values in 'Raw' and 'Norm' columns\n",
    "# This will fill NaN values using the linear interpolation method\n",
    "final_df[raw_cols] = final_df[raw_cols].interpolate(axis=1, method='linear', limit_direction='both')\n",
    "final_df[norm_cols] = final_df[norm_cols].interpolate(axis=1, method='linear', limit_direction='both')\n",
    "\n",
    "# Save the cleaned data back to CSV with interpolation\n",
    "final_df.to_csv(\"river_discharge_data_cleaned.csv\", index=False, encoding='utf-8')\n",
    "print(\" Interpolated data saved as 'river_discharge_data_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564065c",
   "metadata": {},
   "source": [
    "### Preprocessing  Population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e72abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../raw_data/Socioeconomic_data/population_distribution_data_in_climate_vulnerable_areas.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "precipitation_data = pd.read_csv(\"../raw_data/Weather_and_CLimate_data/district-precipitation-trends-with-corresponding-significance-levels (1).csv\")\n",
    "weather_events_data = pd.read_csv(\"../raw_data/Weather_and_CLimate_data/extreme_weather_events.csv\")\n",
    "\n",
    "# Extract relevant columns\n",
    "precipitation_data = precipitation_data[['Districts', 'Annual_Trend']]\n",
    "weather_events_data = weather_events_data[['Location', 'Disaster Type']]\n",
    "\n",
    "# Clean and process the data\n",
    "# Group weather events by district and count occurrences\n",
    "weather_events_data['Location'] = weather_events_data['Location'].str.strip()\n",
    "weather_events_count = weather_events_data.groupby('Location').size().reset_index(name='Extreme_Weather_Events_Count')\n",
    "\n",
    "# Merge the datasets on district/location\n",
    "merged_data = pd.merge(precipitation_data, weather_events_count, left_on='Districts', right_on='Location', how='inner')\n",
    "\n",
    "# Add a placeholder for population distribution (can be replaced with actual data)\n",
    "merged_data['Population'] = 'Unknown'  # Replace with actual population data if available\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "output_file = \"../raw_data/Socioeconomic_data/population_distribution_data_in_climate_vulnerable_areas.csv\"\n",
    "merged_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File '{output_file}' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711abce5",
   "metadata": {},
   "source": [
    "### Preprocessing Climate reanalysis data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60be70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "# Load the NetCDF file\n",
    "ds = xr.open_dataset(\"../raw_data/Weather_and_Climate_data/era5_nepal_temp.nc\")\n",
    "# Select relevant variables\n",
    "vars_of_interest = ['t2m', 'tp', 'smlt', 'lblt', 'src']\n",
    "data = ds[vars_of_interest]\n",
    "\n",
    "# Convert to long-format DataFrame\n",
    "df = data.to_dataframe().reset_index()\n",
    "\n",
    "# Drop rows with any NaN values (or use interpolation if time series)\n",
    "df = df.dropna()\n",
    "\n",
    "# Optional: check for remaining NaNs\n",
    "assert not df.isnull().values.any(), \"Still has missing values!\"\n",
    "\n",
    "# Convert Kelvin to Celsius\n",
    "df['t2m'] = df['t2m'] - 273.15\n",
    "\n",
    "# Normalize each variable (optional - for ML preprocessing)\n",
    "for var in ['t2m', 'tp', 'smlt', 'lblt', 'src']:\n",
    "    df[f'{var}_norm'] = (df[var] - df[var].mean()) / df[var].std()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\n",
    "    'latitude': 'lat',\n",
    "    'longitude': 'lon',\n",
    "    'valid_time': 'date',\n",
    "    't2m': 'temperature_celsius',\n",
    "    'tp': 'total_precip_m',\n",
    "    'smlt': 'snow_melt_m',\n",
    "    'lblt': 'large_scale_snow_melt_m',\n",
    "    'src': 'snow_runoff_m'\n",
    "})\n",
    "\n",
    "# Value range assertions\n",
    "assert df['temperature_celsius'].between(-80, 60).all(), \"Temperature out of range!\"\n",
    "assert df['total_precip_m'].between(0, 1).all(), \"Precipitation seems too high!\"\n",
    "\n",
    "# Duplicate checks\n",
    "assert df.duplicated(subset=['lat', 'lon', 'date']).sum() == 0, \"Duplicate spatial entries!\"\n",
    "\n",
    "df.to_csv(\"../processed_data/processed_era5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b41f28",
   "metadata": {},
   "source": [
    "### Preprocessing Historical temperature and precipatation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8509cb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_14972\\1121377299.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_14972\\1121377299.py:12: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_14972\\1121377299.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Validation ---\n",
      "Null values per column:\n",
      " date           28548\n",
      "year               0\n",
      "month              0\n",
      "district           0\n",
      "lat                0\n",
      "lon                0\n",
      "prectot            0\n",
      "ps                 0\n",
      "qv2m               0\n",
      "rh2m               0\n",
      "t2m                0\n",
      "t2mwet             0\n",
      "t2m_max            0\n",
      "t2m_min            0\n",
      "t2m_range          0\n",
      "ts                 0\n",
      "ws10m              0\n",
      "ws10m_max          0\n",
      "ws10m_min          0\n",
      "ws10m_range        0\n",
      "ws50m              0\n",
      "ws50m_max          0\n",
      "ws50m_min          0\n",
      "ws50m_range        0\n",
      "dtype: int64\n",
      "Date Range: 1981-01-31 00:00:00 to 2019-12-31 00:00:00\n",
      "No negative values found. Validation passed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load the CSV\n",
    "df = pd.read_csv('../raw_data/Weather_and_Climate_data//historic_temp_precipitatio.csv')\n",
    "\n",
    "# column Names and Strip Whitespace\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "#  Missing Values (Interpolate, then Fill)\n",
    "df = df.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "#  Normalize Numeric Data (excluding geolocation or ID columns if present)\n",
    "exclude_columns = ['station', 'district', 'date', 'latitude', 'longitude']\n",
    "numeric_cols = df.select_dtypes(include='number').columns.difference(exclude_columns)\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "#  Temporal Alignment\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y', errors='coerce')\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "#  Check for Georeferenced Structure\n",
    "#  Add or merge latitude/longitude columns if missing\n",
    "# sumes they are present; otherwise you'd merge from a station metadata file\n",
    "\n",
    "#  Convert to Long Format (Optional - for easier analysis)\n",
    "if all(col in df.columns for col in ['date', 'station', 'district']):\n",
    "    df_long = df.melt(id_vars=['date', 'station', 'district'], var_name='parameter', value_name='value')\n",
    "    df_long.to_csv('processed_historic_temp_precipitatio.csv', index=False)\n",
    "\n",
    "#  Save Processed Data\n",
    "df.to_csv('../processed_data/cleaned_historic_temp_precipitation.csv', index=False)\n",
    "\n",
    "#  Validation Checks\n",
    "def validate_dataset(df):\n",
    "    print(\"\\n--- Data Validation ---\")\n",
    "    print(\"Null values per column:\\n\", df.isnull().sum())\n",
    "    if 'date' in df.columns:\n",
    "        print(\"Date Range:\", df['date'].min(), \"to\", df['date'].max())\n",
    "    invalid = df.select_dtypes(include='number') < 0\n",
    "    if invalid.any().any():\n",
    "        print(\"Warning: Negative values detected in numeric columns.\")\n",
    "    else:\n",
    "        print(\"No negative values found. Validation passed.\")\n",
    "\n",
    "validate_dataset(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd367d5",
   "metadata": {},
   "source": [
    "### Preprocessing Infrastructure mapping in flood-prone regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db123973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the shapefiles\n",
    "gdf = gpd.read_file(\"../raw_data/socioeconomic_data/flood prone/data/nepalvdcindicators.shp\")\n",
    "districts = gpd.read_file(\"../raw_data/socioeconomic_data/flood prone/data/district.shp\")\n",
    "\n",
    "# Standardize column names (optional)\n",
    "gdf.columns = gdf.columns.str.strip().str.upper()\n",
    "\n",
    "# Drop irrelevant or duplicate columns (if any)\n",
    "gdf = gdf.loc[:, ~gdf.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebe45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Drop geometry for ML processing\n",
    "df = gdf.drop(columns='GEOMETRY')\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Simple mean imputation (can customize per column)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fad575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "gdf.to_file(\"../processed_data/flood_vulnerability.geojson\", driver=\"GeoJSON\")\n",
    "df.to_csv(\"../processed_data/flood_vulnerability.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcb22d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0\n",
      "Max: 23.0\n",
      "Unique values: [ 0.  1.  3.  5.  2.  7.  6.  4. 13.  8.  9. 17. 14. 12. 23. 10.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", gdf[\"FLOOD_FREQ\"].min())\n",
    "print(\"Max:\", gdf[\"FLOOD_FREQ\"].max())\n",
    "print(\"Unique values:\", gdf[\"FLOOD_FREQ\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee42bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "FLOOD_FREQ out of expected range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gdf\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicates found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Range checks (example)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLOOD_FREQ\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFLOOD_FREQ out of expected range\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: FLOOD_FREQ out of expected range"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "assert gdf.isnull().sum().sum() == 0, \"Missing values found\"\n",
    "\n",
    "# Check for duplicates\n",
    "assert gdf.duplicated().sum() == 0, \"Duplicates found\"\n",
    "\n",
    "# Range checks (example)\n",
    "assert df['FLOOD_FREQ'].between(0, 10).all(), \"FLOOD_FREQ out of expected range\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63c604",
   "metadata": {},
   "source": [
    "### Preprocessing Agriculture yield statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07a5d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../raw_data/Socioeconomic_data/Agriculture Statistics/data/NepalAgriStats_Vegetable.csv\")\n",
    "df.columns = [col.strip() for col in df.columns]  # Clean column names\n",
    "df['DISTRICT_NAME'] = df['DISTRICT_NAME'].str.title()  # Standardize names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b57de21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRICT_CODE    0\n",
      "DISTRICT_NAME    0\n",
      "VG_P_200304      0\n",
      "VG_P_200405      0\n",
      "VG_P_200506      0\n",
      "VG_P_200607      0\n",
      "VG_P_200708      0\n",
      "VG_P_200809      0\n",
      "VG_P_200910      0\n",
      "VG_P_201011      0\n",
      "VG_P_201112      0\n",
      "VG_P_201213      0\n",
      "VG_P_201314      0\n",
      "VG_A_200304      0\n",
      "VG_A_200405      0\n",
      "VG_A_200506      0\n",
      "VG_A_200607      0\n",
      "VG_A_200708      0\n",
      "VG_A_200809      0\n",
      "VG_A_200910      0\n",
      "VG_A_201011      0\n",
      "VG_A_201112      0\n",
      "VG_A_201213      0\n",
      "VG_A_201314      0\n",
      "VG_Y_200304      0\n",
      "VG_Y_200405      0\n",
      "VG_Y_200506      0\n",
      "VG_Y_200607      0\n",
      "VG_Y_200708      0\n",
      "VG_Y_200809      0\n",
      "VG_Y_200910      0\n",
      "VG_Y_201011      0\n",
      "VG_Y_201112      0\n",
      "VG_Y_201213      0\n",
      "VG_Y_201314      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHYAM PANDIT\\AppData\\Local\\Temp\\ipykernel_12492\\546564371.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISTRICT_CODE</th>\n",
       "      <th>DISTRICT_NAME</th>\n",
       "      <th>VG_P_200304</th>\n",
       "      <th>VG_P_200405</th>\n",
       "      <th>VG_P_200506</th>\n",
       "      <th>VG_P_200607</th>\n",
       "      <th>VG_P_200708</th>\n",
       "      <th>VG_P_200809</th>\n",
       "      <th>VG_P_200910</th>\n",
       "      <th>VG_P_201011</th>\n",
       "      <th>...</th>\n",
       "      <th>VG_Y_200405</th>\n",
       "      <th>VG_Y_200506</th>\n",
       "      <th>VG_Y_200607</th>\n",
       "      <th>VG_Y_200708</th>\n",
       "      <th>VG_Y_200809</th>\n",
       "      <th>VG_Y_200910</th>\n",
       "      <th>VG_Y_201011</th>\n",
       "      <th>VG_Y_201112</th>\n",
       "      <th>VG_Y_201213</th>\n",
       "      <th>VG_Y_201314</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Taplejung</td>\n",
       "      <td>6160</td>\n",
       "      <td>6280.0</td>\n",
       "      <td>6659</td>\n",
       "      <td>5920</td>\n",
       "      <td>6063</td>\n",
       "      <td>6565.0</td>\n",
       "      <td>6080.0</td>\n",
       "      <td>17261.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8698.06</td>\n",
       "      <td>8784.96</td>\n",
       "      <td>8314.61</td>\n",
       "      <td>8362.76</td>\n",
       "      <td>8660.95</td>\n",
       "      <td>7937.34</td>\n",
       "      <td>11928.82</td>\n",
       "      <td>11201.3</td>\n",
       "      <td>10789.88</td>\n",
       "      <td>10789.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Panchthar</td>\n",
       "      <td>5850</td>\n",
       "      <td>6943.0</td>\n",
       "      <td>7363</td>\n",
       "      <td>7672</td>\n",
       "      <td>7994</td>\n",
       "      <td>11523.77</td>\n",
       "      <td>11409.65</td>\n",
       "      <td>46577.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8912.71</td>\n",
       "      <td>9012.24</td>\n",
       "      <td>8983.61</td>\n",
       "      <td>8951.85</td>\n",
       "      <td>10779.95</td>\n",
       "      <td>10280.82</td>\n",
       "      <td>13978.69</td>\n",
       "      <td>9891.7</td>\n",
       "      <td>10734.62</td>\n",
       "      <td>10754.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ilam</td>\n",
       "      <td>30000</td>\n",
       "      <td>24916.0</td>\n",
       "      <td>26424</td>\n",
       "      <td>27534</td>\n",
       "      <td>28691</td>\n",
       "      <td>43113.0</td>\n",
       "      <td>44820.0</td>\n",
       "      <td>23694.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9805.59</td>\n",
       "      <td>9904.05</td>\n",
       "      <td>9875.9</td>\n",
       "      <td>9849.3</td>\n",
       "      <td>13318.81</td>\n",
       "      <td>13997.5</td>\n",
       "      <td>15385.71</td>\n",
       "      <td>14135.49</td>\n",
       "      <td>14065.29</td>\n",
       "      <td>14423.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jhapa</td>\n",
       "      <td>50400</td>\n",
       "      <td>52399.0</td>\n",
       "      <td>55567</td>\n",
       "      <td>57901</td>\n",
       "      <td>60330</td>\n",
       "      <td>63312.0</td>\n",
       "      <td>64233.0</td>\n",
       "      <td>101897.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12271.43</td>\n",
       "      <td>12395.05</td>\n",
       "      <td>12358.8</td>\n",
       "      <td>12322.3</td>\n",
       "      <td>12392.25</td>\n",
       "      <td>12352.5</td>\n",
       "      <td>12403.77</td>\n",
       "      <td>16609.67</td>\n",
       "      <td>17073.76</td>\n",
       "      <td>17374.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Sankhuwasabha</td>\n",
       "      <td>11647</td>\n",
       "      <td>12082.0</td>\n",
       "      <td>12812</td>\n",
       "      <td>14770</td>\n",
       "      <td>10756</td>\n",
       "      <td>14502.0</td>\n",
       "      <td>16855.0</td>\n",
       "      <td>7203.8</td>\n",
       "      <td>...</td>\n",
       "      <td>10344.18</td>\n",
       "      <td>10450.24</td>\n",
       "      <td>10075.03</td>\n",
       "      <td>9787.08</td>\n",
       "      <td>10924.29</td>\n",
       "      <td>12044.45</td>\n",
       "      <td>14770.97</td>\n",
       "      <td>9438.45</td>\n",
       "      <td>9270.13</td>\n",
       "      <td>10218.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>Kailali</td>\n",
       "      <td>21900</td>\n",
       "      <td>28278.0</td>\n",
       "      <td>29989</td>\n",
       "      <td>34852</td>\n",
       "      <td>40500</td>\n",
       "      <td>123000.0</td>\n",
       "      <td>124620.0</td>\n",
       "      <td>45896.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13338.68</td>\n",
       "      <td>13472.15</td>\n",
       "      <td>13561.09</td>\n",
       "      <td>13650.15</td>\n",
       "      <td>13666.67</td>\n",
       "      <td>13846.67</td>\n",
       "      <td>12154.66</td>\n",
       "      <td>15061.63</td>\n",
       "      <td>12112.67</td>\n",
       "      <td>15353.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>Darchula</td>\n",
       "      <td>2590</td>\n",
       "      <td>2151.0</td>\n",
       "      <td>2281</td>\n",
       "      <td>2377</td>\n",
       "      <td>11319</td>\n",
       "      <td>10894.1</td>\n",
       "      <td>12587.5</td>\n",
       "      <td>22225.61</td>\n",
       "      <td>...</td>\n",
       "      <td>6938.71</td>\n",
       "      <td>7018.46</td>\n",
       "      <td>6991.18</td>\n",
       "      <td>12618.73</td>\n",
       "      <td>12284.73</td>\n",
       "      <td>12621.58</td>\n",
       "      <td>9033.7</td>\n",
       "      <td>12002.73</td>\n",
       "      <td>11039.71</td>\n",
       "      <td>11112.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>Baitadi</td>\n",
       "      <td>6725</td>\n",
       "      <td>5354.0</td>\n",
       "      <td>5677</td>\n",
       "      <td>8745</td>\n",
       "      <td>8850</td>\n",
       "      <td>10383.75</td>\n",
       "      <td>11459.55</td>\n",
       "      <td>8206.95</td>\n",
       "      <td>...</td>\n",
       "      <td>10237.09</td>\n",
       "      <td>10340.62</td>\n",
       "      <td>10473.05</td>\n",
       "      <td>10172.41</td>\n",
       "      <td>8747.89</td>\n",
       "      <td>9058.93</td>\n",
       "      <td>7233.98</td>\n",
       "      <td>9309.88</td>\n",
       "      <td>12613.72</td>\n",
       "      <td>12228.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>Dadeldhura</td>\n",
       "      <td>6720</td>\n",
       "      <td>5131.0</td>\n",
       "      <td>5441</td>\n",
       "      <td>7839</td>\n",
       "      <td>7900</td>\n",
       "      <td>6664.0</td>\n",
       "      <td>7184.95</td>\n",
       "      <td>10466.975</td>\n",
       "      <td>...</td>\n",
       "      <td>8249.2</td>\n",
       "      <td>8332.31</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>11987.86</td>\n",
       "      <td>6670.67</td>\n",
       "      <td>6637.37</td>\n",
       "      <td>10546.11</td>\n",
       "      <td>12194.24</td>\n",
       "      <td>12357.58</td>\n",
       "      <td>12288.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>Kanchanpur</td>\n",
       "      <td>31984</td>\n",
       "      <td>41479.0</td>\n",
       "      <td>43989</td>\n",
       "      <td>45837</td>\n",
       "      <td>47763</td>\n",
       "      <td>31146.0</td>\n",
       "      <td>40100.0</td>\n",
       "      <td>44109.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12877.68</td>\n",
       "      <td>13006.8</td>\n",
       "      <td>12970.29</td>\n",
       "      <td>12933.39</td>\n",
       "      <td>11600.0</td>\n",
       "      <td>12274.26</td>\n",
       "      <td>13289.02</td>\n",
       "      <td>11271.49</td>\n",
       "      <td>12565.57</td>\n",
       "      <td>12153.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DISTRICT_CODE  DISTRICT_NAME VG_P_200304 VG_P_200405 VG_P_200506  \\\n",
       "0              1      Taplejung        6160      6280.0        6659   \n",
       "1              2      Panchthar        5850      6943.0        7363   \n",
       "2              3           Ilam       30000     24916.0       26424   \n",
       "3              4          Jhapa       50400     52399.0       55567   \n",
       "4              5  Sankhuwasabha       11647     12082.0       12812   \n",
       "..           ...            ...         ...         ...         ...   \n",
       "70            71        Kailali       21900     28278.0       29989   \n",
       "71            72       Darchula        2590      2151.0        2281   \n",
       "72            73        Baitadi        6725      5354.0        5677   \n",
       "73            74     Dadeldhura        6720      5131.0        5441   \n",
       "74            75     Kanchanpur       31984     41479.0       43989   \n",
       "\n",
       "   VG_P_200607 VG_P_200708 VG_P_200809 VG_P_200910 VG_P_201011  ...  \\\n",
       "0         5920        6063      6565.0      6080.0     17261.0  ...   \n",
       "1         7672        7994    11523.77    11409.65     46577.0  ...   \n",
       "2        27534       28691     43113.0     44820.0     23694.0  ...   \n",
       "3        57901       60330     63312.0     64233.0    101897.0  ...   \n",
       "4        14770       10756     14502.0     16855.0      7203.8  ...   \n",
       "..         ...         ...         ...         ...         ...  ...   \n",
       "70       34852       40500    123000.0    124620.0     45896.0  ...   \n",
       "71        2377       11319     10894.1     12587.5    22225.61  ...   \n",
       "72        8745        8850    10383.75    11459.55     8206.95  ...   \n",
       "73        7839        7900      6664.0     7184.95   10466.975  ...   \n",
       "74       45837       47763     31146.0     40100.0     44109.0  ...   \n",
       "\n",
       "   VG_Y_200405 VG_Y_200506 VG_Y_200607 VG_Y_200708 VG_Y_200809 VG_Y_200910  \\\n",
       "0      8698.06     8784.96     8314.61     8362.76     8660.95     7937.34   \n",
       "1      8912.71     9012.24     8983.61     8951.85    10779.95    10280.82   \n",
       "2      9805.59     9904.05      9875.9      9849.3    13318.81     13997.5   \n",
       "3     12271.43    12395.05     12358.8     12322.3    12392.25     12352.5   \n",
       "4     10344.18    10450.24    10075.03     9787.08    10924.29    12044.45   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "70    13338.68    13472.15    13561.09    13650.15    13666.67    13846.67   \n",
       "71     6938.71     7018.46     6991.18    12618.73    12284.73    12621.58   \n",
       "72    10237.09    10340.62    10473.05    10172.41     8747.89     9058.93   \n",
       "73      8249.2     8332.31     13000.0    11987.86     6670.67     6637.37   \n",
       "74    12877.68     13006.8    12970.29    12933.39     11600.0    12274.26   \n",
       "\n",
       "   VG_Y_201011 VG_Y_201112 VG_Y_201213 VG_Y_201314  \n",
       "0     11928.82     11201.3    10789.88    10789.88  \n",
       "1     13978.69      9891.7    10734.62    10754.18  \n",
       "2     15385.71    14135.49    14065.29    14423.76  \n",
       "3     12403.77    16609.67    17073.76    17374.62  \n",
       "4     14770.97     9438.45     9270.13    10218.74  \n",
       "..         ...         ...         ...         ...  \n",
       "70    12154.66    15061.63    12112.67    15353.66  \n",
       "71      9033.7    12002.73    11039.71    11112.05  \n",
       "72     7233.98     9309.88    12613.72    12228.79  \n",
       "73    10546.11    12194.24    12357.58    12288.62  \n",
       "74    13289.02    11271.49    12565.57    12153.14  \n",
       "\n",
       "[75 rows x 35 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values using forward-fill (good for time-series data)\n",
    "df.fillna(method='ffill', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced60f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in ['200304', '200405', '200506', '200607', '200708', '200809',\n",
    "             '200910', '201011', '201112', '201213', '201314']:\n",
    "    df[f'VG_Y_{year}'] = df[f'VG_P_{year}'] / df[f'VG_A_{year}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9276c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned: True\n"
     ]
    }
   ],
   "source": [
    "years = ['200304','200405','200506','200607','200708','200809',\n",
    "         '200910','201011','201112','201213','201314']\n",
    "available_years = sorted(set(col.split('_')[-1] for col in df.columns if col.startswith('VG_P')))\n",
    "print(\"Aligned:\", available_years == years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0a0ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load geo data (GeoJSON or shapefile)\n",
    "geo_df = gpd.read_file(\"../raw_data/Socioeconomic_data/flood prone/data/district.shp\")  # or .shp\n",
    "\n",
    "# Standardize column names\n",
    "geo_df['DNM'] = geo_df['DNM'].str.title().str.strip()\n",
    "df['DISTRICT_NAME'] = df['DISTRICT_NAME'].str.title().str.strip()\n",
    "\n",
    "# Merge using correct district name columns\n",
    "merged_gdf = geo_df.merge(df, left_on='DNM', right_on='DISTRICT_NAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79948936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_geometry = merged_gdf.drop(columns=['geometry'])\n",
    "df_without_geometry.to_csv(\"../processed_data/processed_agriculture_data.csv\", index=False)\n",
    "merged_gdf.to_file(\"../processed_data/processed_agriculture_data.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8faab9",
   "metadata": {},
   "source": [
    "### Preprocessing Weather Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702701aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the raw CSV file\n",
    "df = pd.read_csv(\"../raw_data/Weather_and_Climate_data/meteorological-station-of-nepal.csv\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.replace(\".\", \"\", regex=False)\n",
    "\n",
    "# Remove entirely empty rows or with very few data\n",
    "df = df.dropna(thresh=3)\n",
    "\n",
    "# Strip strings and correct known typos\n",
    "df['District'] = df['District'].str.strip().replace({'Baltadi': 'Baitadi'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb986892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing Index No. with -1 (or a placeholder)\n",
    "df['Index_No'] = df['Index_No'].fillna(-1)\n",
    "\n",
    "# Impute elevation with median of known elevations\n",
    "df['Elemeter'] = df['Elemeter'].fillna(df['Elemeter'].median())\n",
    "\n",
    "# Drop rows where essential data (e.g., Lat/Long) is missing\n",
    "df = df.dropna(subset=['LatLogdegmin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18a7e56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dms_to_dd(dms_str):\n",
    "    \"\"\"Convert deg.min format (e.g., 2941) to decimal degrees\"\"\"\n",
    "    try:\n",
    "        dms_str = str(dms_str).strip()\n",
    "        degrees = int(dms_str[:-2])\n",
    "        minutes = int(dms_str[-2:])\n",
    "        return round(degrees + minutes / 60, 6)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['Latitude'] = df['LatLogdegmin'].apply(lambda x: dms_to_dd(x) if pd.notnull(x) else None)\n",
    "df['Longitude'] = df.iloc[:, 7].apply(lambda x: dms_to_dd(x) if pd.notnull(x) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33aeb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  AgEstdDate NMS_EstdDate\n",
      "0        NaN          NaN\n",
      "1   I MAY 56        26665\n",
      "2    I MAY73        26696\n",
      "3   I MAY 56        26665\n",
      "4   I MAY 56        25720\n",
      "5        NaN        25965\n",
      "6        NaN        25965\n",
      "7        NaN        27061\n",
      "8        NaN        27912\n",
      "9   I JUN 56        26665\n"
     ]
    }
   ],
   "source": [
    "print(df[['AgEstdDate', 'NMS_EstdDate']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090a08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom parser for 'I MAY 56' → '1956-05-01'\n",
    "def parse_ag_date(val):\n",
    "    if pd.isnull(val):\n",
    "        return pd.NaT\n",
    "    match = re.search(r'([A-Z]+)\\s*(\\d{2})', val)\n",
    "    if match:\n",
    "        month_str, year_suffix = match.groups()\n",
    "        try:\n",
    "            return datetime.strptime(f\"{month_str} 19{year_suffix}\", \"%b %Y\")\n",
    "        except:\n",
    "            return pd.NaT\n",
    "    return pd.NaT\n",
    "\n",
    "# Custom parser for '26665' → assume it's a Julian date (example logic)\n",
    "def parse_nms_date(val):\n",
    "    try:\n",
    "        # Assuming it's a Julian day from 1900-01-01\n",
    "        base = datetime(1900, 1, 1)\n",
    "        return base + pd.to_timedelta(int(val), unit=\"D\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply custom parsing\n",
    "df['AgEstdDate'] = df['AgEstdDate'].apply(parse_ag_date)\n",
    "df['NMS_EstdDate'] = df['NMS_EstdDate'].apply(parse_nms_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd884fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Create geometry points\n",
    "geometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")  # WGS84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4fa322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder fields\n",
    "schema_columns = [\n",
    "    'Station_Name', 'Index_No', 'Basin_Office', 'Types_of_Station',\n",
    "    'District', 'Latitude', 'Longitude', 'Elemeter',\n",
    "    'AgEstdDate', 'NMS_EstdDate', 'geometry'\n",
    "]\n",
    "\n",
    "# Save to CSV\n",
    "gdf[schema_columns].to_csv(\"../processed_data/cleaned_weather_stations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be088f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid coordinates found: 1\n",
      "Potential duplicate stations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check coordinate bounds (Nepal approx lat 26°–31°, long 80°–89°)\n",
    "invalid_coords = df[(df['Latitude'] < 26) | (df['Latitude'] > 31) |\n",
    "                    (df['Longitude'] < 80) | (df['Longitude'] > 89)]\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df[df.duplicated(['Station_Name', 'Latitude', 'Longitude'], keep=False)]\n",
    "\n",
    "print(f\"Invalid coordinates found: {len(invalid_coords)}\")\n",
    "print(f\"Potential duplicate stations: {len(duplicates)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
